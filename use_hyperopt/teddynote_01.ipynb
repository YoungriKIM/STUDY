{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[테디노트](https://teddylee777.github.io/thoughts/hyper-opt에서 해보는 HPO\r\n",
    "\r\n",
    "HPO의 종류  \r\n",
    "1. [기본적인 HPO](https://github.com/hyperopt/hyperopt)\r\n",
    "2. [hyperas](https://github.com/maxpumperla/hyperas): hyperopt + keras\r\n",
    "3. [hyperopt-sklearn](https://github.com/hyperopt/hyperopt-sklearn): hyperopt + sklearn\r\n",
    "\r\n",
    "HPO란  \r\n",
    "베이지안 최적화의 접근 방식을 취하는 기술"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 13) (379,)\n"
     ]
    }
   ],
   "source": [
    "# 실습을 위한 boston housing dataset 로드\r\n",
    "from sklearn.datasets import load_boston\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "# 데이터 불러오기\r\n",
    "data = load_boston()\r\n",
    "SEED = 1\r\n",
    "\r\n",
    "# train, test split\r\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['data'], data['target'], random_state = SEED)\r\n",
    "\r\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가함수 정의(RMSE 이용)\r\n",
    "\r\n",
    "from sklearn.metrics import mean_squared_error\r\n",
    "\r\n",
    "def RMSE(y_true, y_pred):\r\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperOpt를 활용한 XGBoost 튜닝 예제  \r\n",
    "\r\n",
    "범위는 space를 만들어 지정해주는데 dict를 만들고 '이름'을 키 값으로 가지고 '하이퍼 파라미터 범위'를 밸류로 갖는다.  \r\n",
    "- hp.choice(하이퍼파라미터 이름, 후보군 리스트): 후보군 리스트 중에서 하나를 선택하여 대입하면서 최적의 하이퍼 파라미터를 찾습니다.  \r\n",
    "- hp.quniform(하이퍼 파라미터 이름, start, end, step): start, end까지 step 간격으로 생성된 후보군 중에서 최적의 하이퍼 파라미터를 찾습니다.  \r\n",
    "- hp.uniform(하이퍼 파라미터 이름, start, end): start, end 사이의 임의의 값 중에서 최적의 하이퍼 파라미터를 찾습니다.  \r\n",
    "\r\n",
    "더 자세한 정보는 [공식 wiki](https://github.com/hyperopt/hyperopt/wiki/FMin)에서 확인 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt\r\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\r\n",
    "from sklearn.metrics import mean_squared_error\r\n",
    "\r\n",
    "# regularization 후보군 정의\r\n",
    "reg_candidate = [1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 5, 10, 100]\r\n",
    "\r\n",
    "# 범위 정의. HPO의 이름을 key로 입력\r\n",
    "space={'max_depth': hp.quniform(\"max_depth\", 5, 15, 1),\r\n",
    "       'learning_rate': hp.quniform ('learning_rate', 0.01, 0.05, 0.005),\r\n",
    "       'reg_alpha' : hp.choice('reg_alpha', reg_candidate),\r\n",
    "       'reg_lambda' : hp.choice('reg_lambda', reg_candidate),\r\n",
    "       'subsample': hp.quniform('subsample', 0.6, 1, 0.05),\r\n",
    "       'colsample_bytree' : hp.quniform('colsample_bytree', 0.6, 1, 0.05),\r\n",
    "       'min_child_weight' : hp.quniform('min_child_weight', 1, 10, 1),\r\n",
    "       'n_estimators': hp.quniform('n_estimators', 200, 1500, 100)\r\n",
    "      }\r\n",
    "\r\n",
    "# 목적 함수 정의\r\n",
    "# n_estimators, max_depth와 같은 반드시 int 타입을 가져야 하는 hyperparamter는 int로 타입 캐스팅 합니다.\r\n",
    "from xgboost import XGBRegressor\r\n",
    "def hyperparameter_tuning(space):\r\n",
    "    model=XGBRegressor(n_estimators =int(space['n_estimators']), \r\n",
    "                       max_depth = int(space['max_depth']), \r\n",
    "                       learning_rate = space['learning_rate'],\r\n",
    "                       reg_alpha = space['reg_alpha'],\r\n",
    "                       reg_lambda = space['reg_lambda'],\r\n",
    "                       subsample = space['subsample'],\r\n",
    "                       colsample_bytree = space['colsample_bytree'], \r\n",
    "                       min_child_weight = int(space['min_child_weight']),\r\n",
    "                       random_state=SEED, \r\n",
    "                      )\r\n",
    "\r\n",
    "    evaluation = [(x_train, y_train), (x_test, y_test)]\r\n",
    "    \r\n",
    "    model.fit(x_train, y_train,\r\n",
    "              eval_set=evaluation, \r\n",
    "              eval_metric=\"rmse\",\r\n",
    "              early_stopping_rounds=20,\r\n",
    "              verbose=0)\r\n",
    "\r\n",
    "    pred = model.predict(x_test)\r\n",
    "    rmse= RMSE(y_test, pred)    \r\n",
    "    # 평가 방식 선정\r\n",
    "    return {'loss':rmse, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:20<00:00,  2.47trial/s, best loss: 2.7623337254527827]\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'min_child_weight': 2, 'n_estimators': 600, 'reg_alpha': 0.01, 'reg_lambda': 0.0001, 'subsample': 0.9, 'random_state': 1}\n"
     ]
    }
   ],
   "source": [
    "# 목적 함수가 지정되었다면 fmin 함수도 최적화를 지정한다.\r\n",
    "# fmin에는 다양한 옵션 값들을 지정할 수 있다. 지정해주는 알고리즘과 최대 반복 횟수 등을 변경해보면서 성능이 달라지는지 모니터링을 한다.\r\n",
    "\r\n",
    "# Trials 객체 선언\r\n",
    "trials = Trials()\r\n",
    "# best에 최적의 하이퍼 파라미터를 return\r\n",
    "best = fmin(fn=hyperparameter_tuning,\r\n",
    "            space=space,\r\n",
    "            algo=tpe.suggest,\r\n",
    "            max_evals=50, # 최대 반복 횟수를 지정합니다.\r\n",
    "            trials=trials)\r\n",
    "\r\n",
    "# 최적화된 결과를 int로 변환해야하는 파라미터는 타입 변환을 수행합니다.\r\n",
    "best['max_depth'] = int(best['max_depth'])\r\n",
    "best['min_child_weight'] = int(best['min_child_weight'])\r\n",
    "best['n_estimators'] = int(best['n_estimators'])\r\n",
    "best['reg_alpha'] = reg_candidate[int(best['reg_alpha'])]\r\n",
    "best['reg_lambda'] = reg_candidate[int(best['reg_lambda'])]\r\n",
    "best['random_state'] = SEED\r\n",
    "print (best)\r\n",
    "# 다음과 같은 결과 확인 가능\r\n",
    "# {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'min_child_weight': 2, \r\n",
    "# 'n_estimators': 600, 'reg_alpha': 0.01, 'reg_lambda': 0.0001, 'subsample': 0.9, 'random_state': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7623337254527827\n"
     ]
    }
   ],
   "source": [
    "# best에 담긴 튜닝된 하이퍼파라미터를 이용해 XGBoost 알고리즘에 적용하여 예측하자\r\n",
    "xgb = XGBRegressor(**best)\r\n",
    "xgb.fit(x_train, y_train)\r\n",
    "pred = xgb.predict(x_test)\r\n",
    "\r\n",
    "print(RMSE(y_test, pred))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b18ecf447b39d25eae6a9ae9d5694327d58d99c869301a8afbbe0bb2ae2074bb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}