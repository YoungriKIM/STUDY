{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dacon_0126_train_1_LSTM.py",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNlekEdLYLUpGlXjy/bKPpR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoungriKIM/STUDY/blob/github/dacon_0126_train_1_LSTM_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_pPZhkioq7n",
        "outputId": "c65429d4-eceb-4fa2-b78c-32acee50f60f"
      },
      "source": [
        "# 0121-5 를 가져와서 0보다 작은 수를 0으로 치환하겠음\r\n",
        "# 섭미션 저장 부분 해결 _ 새로운 기준 파일로 씀\r\n",
        "# 할 때 마다 저장 파일 명 바꿔라~!\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\r\n",
        "from tensorflow.keras.layers import Dense, LSTM, Conv1D, Input, Flatten, MaxPooling1D, Dropout, Reshape, SimpleRNN, LSTM, LeakyReLU, GRU, Conv2D, MaxPool2D\r\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\r\n",
        "from tensorflow.keras.backend import mean, maximum\r\n",
        "import os\r\n",
        "import glob\r\n",
        "import random\r\n",
        "import tensorflow.keras.backend as K\r\n",
        "\r\n",
        "# from google.colab import drive\r\n",
        "# drive.mount('/content/drive')\r\n",
        "\r\n",
        "\r\n",
        "#===================================================================\r\n",
        "# train 데이터 불러옴\r\n",
        "\r\n",
        "# 원하는 열만 가져오기\r\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/colab_data/dacon1/train/train.csv', index_col=None, header=0)\r\n",
        "# print(dataset.shape)\r\n",
        "x_train = dataset.iloc[:,[1,3,4,5,6,7,8]]\r\n",
        "print(x_train.shape)      #(52560, 7)\r\n",
        "\r\n",
        "#===================================================================\r\n",
        "# 81개의 all_test 데이터 48개(1일치) 불러와서 합치기\r\n",
        "def preprocess_data(data):\r\n",
        "    temp = data.copy()\r\n",
        "    return temp.iloc[-48:,[1,3,4,5,6,7,8]]\r\n",
        "\r\n",
        "df_test = []\r\n",
        "\r\n",
        "for i in range(81):\r\n",
        "    file_path = '/content/drive/MyDrive/colab_data/dacon1/test/' + str(i) + '.csv'\r\n",
        "    temp = pd.read_csv(file_path)\r\n",
        "    temp = preprocess_data(temp)\r\n",
        "    df_test.append(temp)\r\n",
        "\r\n",
        "all_test = pd.concat(df_test)\r\n",
        "print(all_test.shape)   #(3888, 7)\r\n",
        "\r\n",
        "\r\n",
        "#===================================================================\r\n",
        "# # GHI라는 기준 추가\r\n",
        "def Add_features(data):\r\n",
        "    data['cos'] = np.cos(np.pi/2 - np.abs(data['Hour'] % 12 - 6)/6 * np.pi/2)\r\n",
        "    # pi = 원주율, abs = 절대값\r\n",
        "    data.insert(1, 'GHI', data['DNI'] * data['cos'] + data['DHI'])\r\n",
        "    # 데이터를 넣어줄건데 1열에(기존 열은 오른쪽으로 밀림), 'GHI'명으로, 마지막의 수식으로 나온 값을\r\n",
        "    data.drop(['cos'], axis=1, inplace = True)\r\n",
        "    #'cos' 열을 삭제를 할 것 이고. 이 삭제한 데이터프레임으로 기존 것을 대체하겠다.\r\n",
        "    return data\r\n",
        "\r\n",
        "x_train = Add_features(x_train)     # 트레인에 붙여줌\r\n",
        "all_test = Add_features(all_test).values    # 테스트에 붙여줌\r\n",
        "\r\n",
        "print(x_train.shape)      #(52560, 8)   #하나씩 붙은 모습\r\n",
        "print(all_test.shape)     #(3888, 8)\r\n",
        "\r\n",
        "#===================================================================\r\n",
        "# train에 다음날, 다다음날의 TARGET을 오른쪽 열으로 붙임\r\n",
        "day_7 = x_train['TARGET'].shift(-48)      #다음날\r\n",
        "day_8 = dataset['TARGET'].shift(-48*2)    #다다음날\r\n",
        "\r\n",
        "x_train = pd.concat([x_train, day_7, day_8], axis=1)\r\n",
        "# dataset2.columns = ['Hour', 'GHI', 'DHI', 'DNI', 'WS', 'RH','T','TARGET','TARGET+1','TARGET+2']\r\n",
        "x_train = x_train.iloc[:-96,:]  # 마지막 2일은 데이터가 비니까 빼준다\r\n",
        "\r\n",
        "print(x_train.shape)       #(52464, 10)       # 8개는 기준일 +GHI / +day7 + day8\r\n",
        "\r\n",
        "#===================================================================\r\n",
        "# x_train을 RNN식으로 데이터 자르기\r\n",
        "aaa = x_train.values\r\n",
        "\r\n",
        "def split_xy(aaa, x_row, x_col, y_row, y_col):\r\n",
        "    x, y = list(), list()\r\n",
        "    for i in range(len(aaa)):\r\n",
        "        if i > len(aaa)-x_row:\r\n",
        "            break\r\n",
        "        tmp_x = aaa[i:i+x_row, :x_col]\r\n",
        "        tmp_y = aaa[i:i+x_row, x_col:x_col+y_col]\r\n",
        "        x.append(tmp_x)\r\n",
        "        y.append(tmp_y)\r\n",
        "    return np.array(x), np.array(y)\r\n",
        "\r\n",
        "# print(x, '\\n\\n', y)\r\n",
        "x_train, y_train = split_xy(aaa, 48,8,48,2)     # 30분씩 RNN식으로 자름\r\n",
        "print(x_train.shape)                    #(52417, 48, 8)\r\n",
        "print(y_train.shape)                    #(52417, 48, 2)\r\n",
        "\r\n",
        "all_test = all_test.reshape(int(all_test.shape[0]/48), 48, all_test.shape[1])\r\n",
        "all_test = all_test.reshape(all_test.shape[0], all_test.shape[1]*all_test.shape[2])\r\n",
        "#===================================================================\r\n",
        "# 데이터 전처리 : 준비 된 데이터 x_train / y_train / all_test\r\n",
        "# 1) 트레인테스트분리 / 2) 민맥스or스탠다드 / 3) 모델에 넣을 쉐잎\r\n",
        "\r\n",
        "# 1) 2차원으로 만들어서 트레인테스트분리\r\n",
        "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2])\r\n",
        "y_train = y_train.reshape(y_train.shape[0], y_train.shape[1]*y_train.shape[2])\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, train_size=0.8, shuffle=True, random_state=311)\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.8, shuffle=True, random_state=311)\r\n",
        "\r\n",
        "# 2) 스탠다드 스케일러\r\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\r\n",
        "scaler = StandardScaler()\r\n",
        "scaler.fit(x_train)\r\n",
        "x_train = scaler.transform(x_train)\r\n",
        "x_val = scaler.transform(x_val)\r\n",
        "x_test = scaler.transform(x_test)\r\n",
        "all_test = scaler.transform(all_test)\r\n",
        "\r\n",
        "# 3) 모델에 넣을 쉐잎\r\n",
        "\r\n",
        "# for conv2D\r\n",
        "num1 = 8\r\n",
        "num2 = 2\r\n",
        "x_train = x_train.reshape(x_train.shape[0], int(x_train.shape[1]/num1), num1)\r\n",
        "x_val = x_val.reshape(x_val.shape[0], int(x_val.shape[1]/num1), num1)\r\n",
        "x_test = x_test.reshape(x_test.shape[0], int(x_test.shape[1]/num1), num1)\r\n",
        "all_test = all_test.reshape(all_test.shape[0], int(all_test.shape[1]/num1), num1)\r\n",
        "\r\n",
        "y_train = y_train.reshape(y_train.shape[0], int(y_train.shape[1]/num2), num2)\r\n",
        "y_val = y_val.reshape(y_val.shape[0], int(y_val.shape[1]/num2), num2)\r\n",
        "y_test = y_test.reshape(y_test.shape[0], int(y_test.shape[1]/num2), num2)\r\n",
        "\r\n",
        "# print(x_train.shape)\r\n",
        "# print(x_val.shape)\r\n",
        "# print(x_test.shape)\r\n",
        "# print(all_test.shape)\r\n",
        "# print(y_train.shape)\r\n",
        "# print(y_val.shape)\r\n",
        "# print(y_test.shape)\r\n",
        "# (33546, 48, 8)\r\n",
        "# (8387, 48, 8)\r\n",
        "# (10484, 48, 8)\r\n",
        "# (81, 48, 8)\r\n",
        "# (33546, 48, 2)\r\n",
        "# (8387, 48, 2)\r\n",
        "# (10484, 48, 2)\r\n",
        "\r\n",
        "#===================================================================\r\n",
        "#퀀타일 로스 적용된 모델 구성 + 컴파일, 훈련까지\r\n",
        "\r\n",
        "def quantile_loss(q, y_true, y_pred):\r\n",
        "    err = (y_true - y_pred)\r\n",
        "    return K.mean(K.maximum(q*err, (q-1)*err), axis=-1)\r\n",
        "# mean = 평균\r\n",
        "# K 를 tensorflow의 백앤드에서 불러왔는데 텐서형식의 mean을 쓰겠다는 것이다.\r\n",
        "\r\n",
        "qlist = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\r\n",
        "subfile = pd.read_csv('/content/drive/MyDrive/colab_data/dacon1/sample_submission.csv')\r\n",
        "\r\n",
        "\r\n",
        "def mymodel():\r\n",
        "    model = Sequential()\r\n",
        "    model.add(LSTM(32, input_shape=(x_train.shape[1], x_train.shape[2]), activation='relu'))\r\n",
        "    model.add(Flatten())\r\n",
        "    model.add(Dense(64))\r\n",
        "    model.add(Dense(64))\r\n",
        "    model.add(Dense(32))\r\n",
        "    model.add(Dense(96))\r\n",
        "    model.add(Reshape((48,2)))\r\n",
        "    model.add(Dense(2))\r\n",
        "    return model\r\n",
        "\r\n",
        "# model.summary()\r\n",
        "\r\n",
        "for q in qlist:\r\n",
        "    patience = 8\r\n",
        "    print(str(q)+'번째 훈련')\r\n",
        "    model = mymodel()\r\n",
        "    model.compile(loss = lambda y_true, y_pred: quantile_loss(q, y_true, y_pred), optimizer='adam', metrics=['mse'])\r\n",
        "    stop = EarlyStopping(monitor ='val_loss', patience=patience, mode='min')\r\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=patience/2, factor=0.5)\r\n",
        "    filepath = f'../data/modelcheckpoint/dacon_train_0122_1_{q:.1f}.hdf5'\r\n",
        "    check = ModelCheckpoint(filepath = filepath, monitor = 'val_loss', save_best_only=True, mode='min') #앞에 f를 붙여준 이유: {}안에 변수를 넣어주겠다는 의미\r\n",
        "    hist = model.fit(x_train, y_train, epochs=500, batch_size=48, verbose=1, validation_split=0.2, callbacks=[stop, reduce_lr])#, check])\r\n",
        "    \r\n",
        "    # 평가, 예측\r\n",
        "    result = model.evaluate(x_test, y_test, batch_size=48)\r\n",
        "    print('loss: ', result[0])\r\n",
        "    print('mae: ', result[1])\r\n",
        "    y_predict = model.predict(all_test)\r\n",
        "    # print(y_predict.shape)  #(81, 48, 2)\r\n",
        "    \r\n",
        "    # 예측값을 submission에 넣기\r\n",
        "    y_predict = pd.DataFrame(y_predict.reshape(y_predict.shape[0]*y_predict.shape[1],y_predict.shape[2]))\r\n",
        "    y_predict2 = pd.concat([y_predict], axis=1)\r\n",
        "    y_predict2[y_predict<0] = 0\r\n",
        "    y_predict3 = y_predict2.to_numpy()\r\n",
        "        \r\n",
        "    print(str(q)+'번째 지정')\r\n",
        "    subfile.loc[subfile.id.str.contains('Day7'), 'q_' + str(q)] = y_predict3[:,0].round(2)\r\n",
        "    subfile.loc[subfile.id.str.contains('Day8'), 'q_' + str(q)] = y_predict3[:,1].round(2)\r\n",
        "\r\n",
        "    # print(subfile.head())\r\n",
        "\r\n",
        "subfile.to_csv('/content/drive/MyDrive/colab_data/dacon1/sub_0126_1.csv', index=False)\r\n",
        "\r\n",
        "\r\n",
        "#===================================================================\r\n",
        "print('(ง˙∇˙)ว {오늘 안에 조지고만다!!!]')\r\n",
        "\r\n",
        "# 2.6781342257\tlstm trash^^"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(52560, 7)\n",
            "(3888, 7)\n",
            "(52560, 8)\n",
            "(3888, 8)\n",
            "(52464, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(52417, 48, 8)\n",
            "(52417, 48, 2)\n",
            "0.1번째 훈련\n",
            "Epoch 1/500\n",
            "560/560 [==============================] - 14s 22ms/step - loss: 1.7585 - mse: 933.1550 - val_loss: 1.5411 - val_mse: 591.5095\n",
            "Epoch 2/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.4905 - mse: 518.4033 - val_loss: 1.4603 - val_mse: 523.7208\n",
            "Epoch 3/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.4540 - mse: 499.8935 - val_loss: 1.4232 - val_mse: 498.7765\n",
            "Epoch 4/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.4059 - mse: 446.2602 - val_loss: 1.4018 - val_mse: 448.2414\n",
            "Epoch 5/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3952 - mse: 442.4872 - val_loss: 1.4021 - val_mse: 448.9741\n",
            "Epoch 6/500\n",
            "560/560 [==============================] - 14s 25ms/step - loss: 1.3823 - mse: 436.0341 - val_loss: 1.3835 - val_mse: 429.1865\n",
            "Epoch 7/500\n",
            "560/560 [==============================] - 14s 25ms/step - loss: 1.3719 - mse: 428.3946 - val_loss: 1.3676 - val_mse: 440.6447\n",
            "Epoch 8/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.3622 - mse: 422.0125 - val_loss: 1.3554 - val_mse: 398.5211\n",
            "Epoch 9/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.3607 - mse: 424.1540 - val_loss: 1.3605 - val_mse: 409.8537\n",
            "Epoch 10/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.3475 - mse: 417.5672 - val_loss: 1.3601 - val_mse: 440.9900\n",
            "Epoch 11/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.3394 - mse: 410.9195 - val_loss: 1.3405 - val_mse: 423.2578\n",
            "Epoch 12/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.3343 - mse: 412.9563 - val_loss: 1.3460 - val_mse: 427.2381\n",
            "Epoch 13/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3209 - mse: 401.6127 - val_loss: 1.3222 - val_mse: 391.9004\n",
            "Epoch 14/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3164 - mse: 401.2617 - val_loss: 1.3310 - val_mse: 358.9416\n",
            "Epoch 15/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3084 - mse: 393.6529 - val_loss: 1.3149 - val_mse: 386.9356\n",
            "Epoch 16/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3063 - mse: 394.5441 - val_loss: 1.3199 - val_mse: 442.2437\n",
            "Epoch 17/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.2953 - mse: 390.7552 - val_loss: 1.3083 - val_mse: 369.6379\n",
            "Epoch 18/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.2887 - mse: 385.7936 - val_loss: 1.2984 - val_mse: 401.1516\n",
            "Epoch 19/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.2791 - mse: 381.9571 - val_loss: 1.2822 - val_mse: 391.2362\n",
            "Epoch 20/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.2819 - mse: 381.8445 - val_loss: 1.2972 - val_mse: 343.9417\n",
            "Epoch 21/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.2784 - mse: 377.0829 - val_loss: 1.2805 - val_mse: 371.1500\n",
            "Epoch 22/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.2686 - mse: 372.2176 - val_loss: 1.2749 - val_mse: 366.0790\n",
            "Epoch 23/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.2558 - mse: 367.5478 - val_loss: 1.2803 - val_mse: 400.0770\n",
            "Epoch 24/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.2724 - mse: 380.4677 - val_loss: 1.2923 - val_mse: 388.2927\n",
            "Epoch 25/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.2681 - mse: 373.7521 - val_loss: 1.2735 - val_mse: 364.7575\n",
            "Epoch 26/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.2524 - mse: 366.4459 - val_loss: 1.2552 - val_mse: 368.9881\n",
            "Epoch 27/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.2459 - mse: 364.0642 - val_loss: 1.2638 - val_mse: 388.5240\n",
            "Epoch 28/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.2478 - mse: 366.2016 - val_loss: 1.2510 - val_mse: 379.1702\n",
            "Epoch 29/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.2448 - mse: 365.4384 - val_loss: 1.2633 - val_mse: 403.0602\n",
            "Epoch 30/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.2338 - mse: 357.6563 - val_loss: 1.2451 - val_mse: 370.4708\n",
            "Epoch 31/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.2386 - mse: 364.5006 - val_loss: 1.2394 - val_mse: 339.8727\n",
            "Epoch 32/500\n",
            "560/560 [==============================] - 14s 25ms/step - loss: 1.2244 - mse: 356.2085 - val_loss: 1.2355 - val_mse: 368.7713\n",
            "Epoch 33/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.2295 - mse: 361.1990 - val_loss: 1.2336 - val_mse: 362.5657\n",
            "Epoch 34/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.2233 - mse: 356.1705 - val_loss: 1.2416 - val_mse: 353.8574\n",
            "Epoch 35/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.2189 - mse: 355.5438 - val_loss: 1.2281 - val_mse: 321.4474\n",
            "Epoch 36/500\n",
            "560/560 [==============================] - 14s 24ms/step - loss: 1.2126 - mse: 351.1489 - val_loss: 1.2183 - val_mse: 341.2513\n",
            "Epoch 37/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.2101 - mse: 349.0766 - val_loss: 1.2299 - val_mse: 382.7105\n",
            "Epoch 38/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.2027 - mse: 349.0693 - val_loss: 1.2151 - val_mse: 366.7074\n",
            "Epoch 39/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.1992 - mse: 343.6574 - val_loss: 1.2356 - val_mse: 343.7006\n",
            "Epoch 40/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.2036 - mse: 350.6455 - val_loss: 1.2120 - val_mse: 360.6725\n",
            "Epoch 41/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.1947 - mse: 346.0475 - val_loss: 1.2015 - val_mse: 343.8185\n",
            "Epoch 42/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.1918 - mse: 344.1225 - val_loss: 1.2067 - val_mse: 354.8546\n",
            "Epoch 43/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.1845 - mse: 338.7436 - val_loss: 1.2105 - val_mse: 371.4070\n",
            "Epoch 44/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.1868 - mse: 342.0870 - val_loss: 1.2007 - val_mse: 359.2450\n",
            "Epoch 45/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.1835 - mse: 339.6408 - val_loss: 1.2266 - val_mse: 382.4830\n",
            "Epoch 46/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.1778 - mse: 337.2135 - val_loss: 1.1886 - val_mse: 343.9581\n",
            "Epoch 47/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.1856 - mse: 342.0416 - val_loss: 1.1861 - val_mse: 344.9969\n",
            "Epoch 48/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.1881 - mse: 344.6541 - val_loss: 1.1895 - val_mse: 340.2326\n",
            "Epoch 49/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.1719 - mse: 335.6704 - val_loss: 1.1840 - val_mse: 347.9923\n",
            "Epoch 50/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.1738 - mse: 336.1246 - val_loss: 1.1854 - val_mse: 358.5706\n",
            "Epoch 51/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.1706 - mse: 334.1358 - val_loss: 1.1873 - val_mse: 343.9127\n",
            "Epoch 52/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.1602 - mse: 329.8067 - val_loss: 1.1811 - val_mse: 352.5760\n",
            "Epoch 53/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.1669 - mse: 334.6018 - val_loss: 1.1701 - val_mse: 315.5262\n",
            "Epoch 54/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.1670 - mse: 331.4707 - val_loss: 1.1989 - val_mse: 352.8863\n",
            "Epoch 55/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.1667 - mse: 335.7603 - val_loss: 1.1936 - val_mse: 396.7484\n",
            "Epoch 56/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.1612 - mse: 333.4577 - val_loss: 1.1691 - val_mse: 348.7508\n",
            "Epoch 57/500\n",
            "560/560 [==============================] - 15s 27ms/step - loss: 1.1570 - mse: 329.4455 - val_loss: 1.1668 - val_mse: 328.2546\n",
            "Epoch 58/500\n",
            "560/560 [==============================] - 14s 26ms/step - loss: 1.1468 - mse: 323.4218 - val_loss: 1.1730 - val_mse: 324.1925\n",
            "Epoch 59/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.1616 - mse: 334.0721 - val_loss: 1.1648 - val_mse: 336.3143\n",
            "Epoch 60/500\n",
            "560/560 [==============================] - 14s 25ms/step - loss: 1.1515 - mse: 327.6648 - val_loss: 1.1679 - val_mse: 350.4853\n",
            "Epoch 61/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.1538 - mse: 329.2671 - val_loss: 1.1662 - val_mse: 353.4907\n",
            "Epoch 62/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.1499 - mse: 328.5578 - val_loss: 1.1700 - val_mse: 361.6106\n",
            "Epoch 63/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.1447 - mse: 328.3053 - val_loss: 1.1580 - val_mse: 327.3267\n",
            "Epoch 64/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.1541 - mse: 331.9287 - val_loss: 1.1724 - val_mse: 318.5482\n",
            "Epoch 65/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.1482 - mse: 326.0855 - val_loss: 1.1563 - val_mse: 338.6685\n",
            "Epoch 66/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.1474 - mse: 326.7128 - val_loss: 1.1890 - val_mse: 320.4466\n",
            "Epoch 67/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.1467 - mse: 326.0817 - val_loss: 1.1496 - val_mse: 336.1823\n",
            "Epoch 68/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.1446 - mse: 328.1192 - val_loss: 1.1507 - val_mse: 317.7566\n",
            "Epoch 69/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.1400 - mse: 325.1576 - val_loss: 1.1549 - val_mse: 310.6718\n",
            "Epoch 70/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.1361 - mse: 322.9634 - val_loss: 1.1578 - val_mse: 319.1071\n",
            "Epoch 71/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.1463 - mse: 324.2236 - val_loss: 1.1515 - val_mse: 307.4915\n",
            "Epoch 72/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.1114 - mse: 314.4127 - val_loss: 1.1275 - val_mse: 320.6925\n",
            "Epoch 73/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.1116 - mse: 316.3762 - val_loss: 1.1264 - val_mse: 318.7356\n",
            "Epoch 74/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.1133 - mse: 316.8897 - val_loss: 1.1272 - val_mse: 309.5798\n",
            "Epoch 75/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.1174 - mse: 318.6260 - val_loss: 1.1289 - val_mse: 312.1678\n",
            "Epoch 76/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.1104 - mse: 315.0037 - val_loss: 1.1243 - val_mse: 334.0491\n",
            "Epoch 77/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.1026 - mse: 313.1307 - val_loss: 1.2292 - val_mse: 319.0283\n",
            "Epoch 78/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.1396 - mse: 326.1971 - val_loss: 1.1260 - val_mse: 326.6862\n",
            "Epoch 79/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.1099 - mse: 315.4617 - val_loss: 1.1202 - val_mse: 319.3442\n",
            "Epoch 80/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.0992 - mse: 311.6388 - val_loss: 1.1188 - val_mse: 311.0910\n",
            "Epoch 81/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.1084 - mse: 315.8102 - val_loss: 1.1240 - val_mse: 318.8842\n",
            "Epoch 82/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.1074 - mse: 314.1606 - val_loss: 1.1223 - val_mse: 323.0302\n",
            "Epoch 83/500\n",
            "560/560 [==============================] - 14s 25ms/step - loss: 1.1093 - mse: 316.4466 - val_loss: 1.1172 - val_mse: 310.3563\n",
            "Epoch 84/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.1080 - mse: 315.6154 - val_loss: 1.1217 - val_mse: 313.4048\n",
            "Epoch 85/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.1009 - mse: 313.2850 - val_loss: 1.1197 - val_mse: 322.6064\n",
            "Epoch 86/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.1047 - mse: 314.2133 - val_loss: 1.1359 - val_mse: 296.7729\n",
            "Epoch 87/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.1091 - mse: 313.6416 - val_loss: 1.1222 - val_mse: 328.7516\n",
            "Epoch 88/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.0938 - mse: 313.1104 - val_loss: 1.1086 - val_mse: 316.0670\n",
            "Epoch 89/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.0848 - mse: 308.4439 - val_loss: 1.1082 - val_mse: 311.6111\n",
            "Epoch 90/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.0914 - mse: 311.6063 - val_loss: 1.1073 - val_mse: 318.4072\n",
            "Epoch 91/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.0865 - mse: 309.2645 - val_loss: 1.1081 - val_mse: 324.5516\n",
            "Epoch 92/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.0889 - mse: 310.0098 - val_loss: 1.1086 - val_mse: 314.4114\n",
            "Epoch 93/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.0906 - mse: 311.7633 - val_loss: 1.1078 - val_mse: 319.6414\n",
            "Epoch 94/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.0916 - mse: 311.1914 - val_loss: 1.1074 - val_mse: 308.9753\n",
            "Epoch 95/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.0901 - mse: 311.8881 - val_loss: 1.1019 - val_mse: 317.4897\n",
            "Epoch 96/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.0841 - mse: 309.9647 - val_loss: 1.1014 - val_mse: 316.4791\n",
            "Epoch 97/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.0865 - mse: 312.5689 - val_loss: 1.1020 - val_mse: 308.1503\n",
            "Epoch 98/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.0833 - mse: 309.1972 - val_loss: 1.1016 - val_mse: 315.9897\n",
            "Epoch 99/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.0850 - mse: 310.2620 - val_loss: 1.1021 - val_mse: 311.2698\n",
            "Epoch 100/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.0812 - mse: 308.3530 - val_loss: 1.1013 - val_mse: 314.0143\n",
            "Epoch 101/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.0772 - mse: 307.6302 - val_loss: 1.0992 - val_mse: 308.6839\n",
            "Epoch 102/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.0756 - mse: 305.8893 - val_loss: 1.0986 - val_mse: 314.4099\n",
            "Epoch 103/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.0841 - mse: 311.7830 - val_loss: 1.1001 - val_mse: 307.4218\n",
            "Epoch 104/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.0760 - mse: 307.3845 - val_loss: 1.0994 - val_mse: 310.0136\n",
            "Epoch 105/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.0761 - mse: 306.0307 - val_loss: 1.0981 - val_mse: 313.6012\n",
            "Epoch 106/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.0798 - mse: 309.3202 - val_loss: 1.0986 - val_mse: 311.5559\n",
            "Epoch 107/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.0749 - mse: 307.6184 - val_loss: 1.0983 - val_mse: 311.2458\n",
            "Epoch 108/500\n",
            "560/560 [==============================] - 14s 25ms/step - loss: 1.0800 - mse: 308.7898 - val_loss: 1.0992 - val_mse: 309.0258\n",
            "Epoch 109/500\n",
            "560/560 [==============================] - 15s 28ms/step - loss: 1.0739 - mse: 305.5582 - val_loss: 1.0976 - val_mse: 312.4653\n",
            "Epoch 110/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.0804 - mse: 308.1791 - val_loss: 1.0982 - val_mse: 309.6631\n",
            "Epoch 111/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.0808 - mse: 308.8697 - val_loss: 1.0980 - val_mse: 313.1722\n",
            "Epoch 112/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.0722 - mse: 303.8866 - val_loss: 1.0976 - val_mse: 312.9943\n",
            "Epoch 113/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.0728 - mse: 303.7857 - val_loss: 1.0972 - val_mse: 314.4125\n",
            "Epoch 114/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.0730 - mse: 305.7488 - val_loss: 1.0985 - val_mse: 313.7999\n",
            "Epoch 115/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.0755 - mse: 306.6902 - val_loss: 1.0977 - val_mse: 312.5960\n",
            "Epoch 116/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.0782 - mse: 308.5540 - val_loss: 1.0985 - val_mse: 307.1948\n",
            "Epoch 117/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.0821 - mse: 309.3572 - val_loss: 1.0975 - val_mse: 314.1505\n",
            "Epoch 118/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.0749 - mse: 305.2184 - val_loss: 1.0961 - val_mse: 313.8217\n",
            "Epoch 119/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.0701 - mse: 304.8700 - val_loss: 1.0966 - val_mse: 310.4348\n",
            "Epoch 120/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.0737 - mse: 308.2469 - val_loss: 1.0965 - val_mse: 310.9783\n",
            "Epoch 121/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.0700 - mse: 304.6598 - val_loss: 1.0962 - val_mse: 312.7007\n",
            "Epoch 122/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.0766 - mse: 308.8713 - val_loss: 1.0962 - val_mse: 311.1624\n",
            "Epoch 123/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.0737 - mse: 306.1754 - val_loss: 1.0955 - val_mse: 313.1079\n",
            "Epoch 124/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.0707 - mse: 304.4741 - val_loss: 1.0960 - val_mse: 309.7605\n",
            "Epoch 125/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.0729 - mse: 305.5495 - val_loss: 1.0959 - val_mse: 310.9699\n",
            "Epoch 126/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.0743 - mse: 306.8328 - val_loss: 1.0956 - val_mse: 314.1518\n",
            "Epoch 127/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.0682 - mse: 305.2338 - val_loss: 1.0957 - val_mse: 311.7356\n",
            "Epoch 128/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.0635 - mse: 302.0344 - val_loss: 1.0955 - val_mse: 310.9430\n",
            "Epoch 129/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.0658 - mse: 302.9697 - val_loss: 1.0954 - val_mse: 310.7892\n",
            "Epoch 130/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.0676 - mse: 306.1507 - val_loss: 1.0956 - val_mse: 310.2104\n",
            "Epoch 131/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.0705 - mse: 305.2105 - val_loss: 1.0956 - val_mse: 310.3868\n",
            "Epoch 132/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.0699 - mse: 305.6313 - val_loss: 1.0954 - val_mse: 310.8344\n",
            "Epoch 133/500\n",
            "560/560 [==============================] - 15s 26ms/step - loss: 1.0728 - mse: 307.4730 - val_loss: 1.0953 - val_mse: 311.6022\n",
            "Epoch 134/500\n",
            "560/560 [==============================] - 14s 24ms/step - loss: 1.0784 - mse: 309.8783 - val_loss: 1.0954 - val_mse: 311.1793\n",
            "Epoch 135/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.0719 - mse: 305.2419 - val_loss: 1.0955 - val_mse: 310.4486\n",
            "Epoch 136/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.0755 - mse: 306.6788 - val_loss: 1.0954 - val_mse: 311.1019\n",
            "Epoch 137/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.0695 - mse: 305.0100 - val_loss: 1.0954 - val_mse: 310.5663\n",
            "Epoch 138/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.0671 - mse: 304.4704 - val_loss: 1.0954 - val_mse: 310.8237\n",
            "Epoch 139/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.0754 - mse: 307.8108 - val_loss: 1.0953 - val_mse: 311.2553\n",
            "Epoch 140/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.0677 - mse: 302.3605 - val_loss: 1.0954 - val_mse: 310.8781\n",
            "Epoch 141/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.0720 - mse: 306.9017 - val_loss: 1.0954 - val_mse: 310.8193\n",
            "219/219 [==============================] - 1s 6ms/step - loss: 1.0895 - mse: 300.6362\n",
            "loss:  1.0894540548324585\n",
            "mae:  300.63623046875\n",
            "0.1번째 지정\n",
            "0.2번째 훈련\n",
            "Epoch 1/500\n",
            "560/560 [==============================] - 14s 22ms/step - loss: 3.3036 - mse: 787.1689 - val_loss: 2.5285 - val_mse: 279.1645\n",
            "Epoch 2/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.4479 - mse: 278.2113 - val_loss: 2.3361 - val_mse: 247.0995\n",
            "Epoch 3/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3428 - mse: 255.1040 - val_loss: 2.3092 - val_mse: 247.1098\n",
            "Epoch 4/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2995 - mse: 249.5870 - val_loss: 2.2764 - val_mse: 239.5752\n",
            "Epoch 5/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2828 - mse: 246.6570 - val_loss: 2.2522 - val_mse: 227.1505\n",
            "Epoch 6/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2553 - mse: 241.9545 - val_loss: 2.2844 - val_mse: 210.3060\n",
            "Epoch 7/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.2537 - mse: 241.1998 - val_loss: 2.2471 - val_mse: 262.3548\n",
            "Epoch 8/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.2167 - mse: 236.5715 - val_loss: 2.2047 - val_mse: 220.4691\n",
            "Epoch 9/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 2.2092 - mse: 233.6884 - val_loss: 2.2087 - val_mse: 244.8669\n",
            "Epoch 10/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 2.1919 - mse: 231.6686 - val_loss: 2.1975 - val_mse: 243.7671\n",
            "Epoch 11/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.1786 - mse: 230.3807 - val_loss: 2.1652 - val_mse: 230.0931\n",
            "Epoch 12/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.1672 - mse: 225.9587 - val_loss: 2.1999 - val_mse: 199.1866\n",
            "Epoch 13/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.1533 - mse: 222.5403 - val_loss: 2.1659 - val_mse: 198.9101\n",
            "Epoch 14/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.1499 - mse: 222.3527 - val_loss: 2.1487 - val_mse: 239.4588\n",
            "Epoch 15/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.1380 - mse: 224.6044 - val_loss: 2.1246 - val_mse: 210.7890\n",
            "Epoch 16/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 2.1270 - mse: 220.4975 - val_loss: 2.1228 - val_mse: 244.2081\n",
            "Epoch 17/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.1139 - mse: 220.5267 - val_loss: 2.1099 - val_mse: 206.9551\n",
            "Epoch 18/500\n",
            "560/560 [==============================] - 15s 26ms/step - loss: 2.0966 - mse: 217.7183 - val_loss: 2.1153 - val_mse: 225.5966\n",
            "Epoch 19/500\n",
            "560/560 [==============================] - 14s 25ms/step - loss: 2.0844 - mse: 214.3242 - val_loss: 2.0806 - val_mse: 222.6182\n",
            "Epoch 20/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.0725 - mse: 212.2192 - val_loss: 2.0936 - val_mse: 229.2467\n",
            "Epoch 21/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 2.0664 - mse: 210.6961 - val_loss: 2.0727 - val_mse: 211.7845\n",
            "Epoch 22/500\n",
            "560/560 [==============================] - 14s 24ms/step - loss: 2.0528 - mse: 208.9102 - val_loss: 2.0641 - val_mse: 197.3103\n",
            "Epoch 23/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.0554 - mse: 209.2784 - val_loss: 2.0540 - val_mse: 227.7946\n",
            "Epoch 24/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.0486 - mse: 211.8817 - val_loss: 2.0643 - val_mse: 239.6282\n",
            "Epoch 25/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.0396 - mse: 208.9740 - val_loss: 2.0242 - val_mse: 201.1757\n",
            "Epoch 26/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.0269 - mse: 206.4180 - val_loss: 2.0266 - val_mse: 190.2834\n",
            "Epoch 27/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.0158 - mse: 203.5894 - val_loss: 2.0404 - val_mse: 192.0460\n",
            "Epoch 28/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.0064 - mse: 201.1963 - val_loss: 2.0208 - val_mse: 200.9701\n",
            "Epoch 29/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.0098 - mse: 203.0426 - val_loss: 2.0183 - val_mse: 181.8215\n",
            "Epoch 30/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.0015 - mse: 201.1270 - val_loss: 1.9984 - val_mse: 217.3081\n",
            "Epoch 31/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.9796 - mse: 198.0434 - val_loss: 1.9884 - val_mse: 181.3389\n",
            "Epoch 32/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.9769 - mse: 196.6658 - val_loss: 1.9949 - val_mse: 206.1882\n",
            "Epoch 33/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.9677 - mse: 196.5606 - val_loss: 1.9830 - val_mse: 220.6858\n",
            "Epoch 34/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.9710 - mse: 197.4978 - val_loss: 1.9813 - val_mse: 206.2775\n",
            "Epoch 35/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.9542 - mse: 192.9661 - val_loss: 1.9834 - val_mse: 176.3901\n",
            "Epoch 36/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.9515 - mse: 191.6052 - val_loss: 1.9667 - val_mse: 212.2830\n",
            "Epoch 37/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.9495 - mse: 194.7532 - val_loss: 1.9592 - val_mse: 213.6730\n",
            "Epoch 38/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.9588 - mse: 196.5266 - val_loss: 2.0822 - val_mse: 234.2149\n",
            "Epoch 39/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.0337 - mse: 209.1427 - val_loss: 1.9929 - val_mse: 175.8927\n",
            "Epoch 40/500\n",
            "560/560 [==============================] - 14s 24ms/step - loss: 1.9639 - mse: 194.0328 - val_loss: 1.9685 - val_mse: 204.2543\n",
            "Epoch 41/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.9378 - mse: 190.3561 - val_loss: 1.9420 - val_mse: 193.2965\n",
            "Epoch 42/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.9364 - mse: 190.3690 - val_loss: 1.9365 - val_mse: 184.9781\n",
            "Epoch 43/500\n",
            "560/560 [==============================] - 14s 26ms/step - loss: 1.9226 - mse: 189.0089 - val_loss: 1.9412 - val_mse: 188.4460\n",
            "Epoch 44/500\n",
            "560/560 [==============================] - 14s 25ms/step - loss: 1.9179 - mse: 188.9992 - val_loss: 1.9291 - val_mse: 189.8991\n",
            "Epoch 45/500\n",
            "560/560 [==============================] - 14s 24ms/step - loss: 1.9092 - mse: 187.2351 - val_loss: 1.9161 - val_mse: 188.0192\n",
            "Epoch 46/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.9037 - mse: 186.3119 - val_loss: 1.9060 - val_mse: 191.2574\n",
            "Epoch 47/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.9113 - mse: 186.7851 - val_loss: 1.9202 - val_mse: 177.3309\n",
            "Epoch 48/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.9008 - mse: 186.2248 - val_loss: 1.9170 - val_mse: 179.4445\n",
            "Epoch 49/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8937 - mse: 185.9280 - val_loss: 1.9193 - val_mse: 176.1024\n",
            "Epoch 50/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.9103 - mse: 187.9940 - val_loss: 1.9284 - val_mse: 213.1704\n",
            "Epoch 51/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.8698 - mse: 183.0982 - val_loss: 1.8838 - val_mse: 184.0132\n",
            "Epoch 52/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8580 - mse: 181.7185 - val_loss: 1.8810 - val_mse: 192.3829\n",
            "Epoch 53/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.8484 - mse: 182.0863 - val_loss: 1.8736 - val_mse: 188.1540\n",
            "Epoch 54/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8450 - mse: 180.1188 - val_loss: 1.8703 - val_mse: 184.0048\n",
            "Epoch 55/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.8464 - mse: 181.1715 - val_loss: 1.8837 - val_mse: 197.7299\n",
            "Epoch 56/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.8453 - mse: 180.6756 - val_loss: 1.8752 - val_mse: 183.6837\n",
            "Epoch 57/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.8441 - mse: 180.6711 - val_loss: 1.8621 - val_mse: 178.6333\n",
            "Epoch 58/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8397 - mse: 180.6455 - val_loss: 1.8618 - val_mse: 184.7153\n",
            "Epoch 59/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8527 - mse: 183.6805 - val_loss: 1.8630 - val_mse: 180.0721\n",
            "Epoch 60/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8431 - mse: 181.7511 - val_loss: 1.8542 - val_mse: 180.9543\n",
            "Epoch 61/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.8391 - mse: 181.1389 - val_loss: 1.8558 - val_mse: 173.6772\n",
            "Epoch 62/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8302 - mse: 179.8174 - val_loss: 1.8559 - val_mse: 185.8029\n",
            "Epoch 63/500\n",
            "560/560 [==============================] - 14s 25ms/step - loss: 1.8369 - mse: 181.8913 - val_loss: 1.8512 - val_mse: 174.2838\n",
            "Epoch 64/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.8392 - mse: 181.5092 - val_loss: 1.8494 - val_mse: 187.9033\n",
            "Epoch 65/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8192 - mse: 177.8200 - val_loss: 1.8636 - val_mse: 181.1943\n",
            "Epoch 66/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8221 - mse: 178.2977 - val_loss: 1.8491 - val_mse: 179.5070\n",
            "Epoch 67/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8118 - mse: 175.8397 - val_loss: 1.8546 - val_mse: 186.5309\n",
            "Epoch 68/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.8257 - mse: 178.0946 - val_loss: 1.8656 - val_mse: 192.5269\n",
            "Epoch 69/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8214 - mse: 178.6316 - val_loss: 1.8477 - val_mse: 177.2721\n",
            "Epoch 70/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8269 - mse: 180.5473 - val_loss: 1.8483 - val_mse: 174.6255\n",
            "Epoch 71/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8191 - mse: 178.1138 - val_loss: 1.8624 - val_mse: 193.9946\n",
            "Epoch 72/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8204 - mse: 178.4016 - val_loss: 1.8577 - val_mse: 165.6309\n",
            "Epoch 73/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.8203 - mse: 178.0922 - val_loss: 1.8483 - val_mse: 180.6177\n",
            "Epoch 74/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7886 - mse: 174.8619 - val_loss: 1.8354 - val_mse: 189.8228\n",
            "Epoch 75/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.7954 - mse: 175.8886 - val_loss: 1.8259 - val_mse: 178.5992\n",
            "Epoch 76/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8050 - mse: 177.7425 - val_loss: 1.8265 - val_mse: 173.2289\n",
            "Epoch 77/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7943 - mse: 175.2378 - val_loss: 1.8296 - val_mse: 175.8715\n",
            "Epoch 78/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7840 - mse: 173.4654 - val_loss: 1.8241 - val_mse: 185.3015\n",
            "Epoch 79/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7975 - mse: 176.3543 - val_loss: 1.8268 - val_mse: 177.0787\n",
            "Epoch 80/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7830 - mse: 174.5432 - val_loss: 1.8315 - val_mse: 173.9552\n",
            "Epoch 81/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7876 - mse: 175.6015 - val_loss: 1.8201 - val_mse: 174.4324\n",
            "Epoch 82/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7827 - mse: 173.4284 - val_loss: 1.8306 - val_mse: 193.4865\n",
            "Epoch 83/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7890 - mse: 175.8236 - val_loss: 1.8221 - val_mse: 181.4757\n",
            "Epoch 84/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7900 - mse: 175.9353 - val_loss: 1.8209 - val_mse: 184.2517\n",
            "Epoch 85/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7872 - mse: 175.4800 - val_loss: 1.8163 - val_mse: 178.7444\n",
            "Epoch 86/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.7846 - mse: 175.5494 - val_loss: 1.8226 - val_mse: 173.8956\n",
            "Epoch 87/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.7858 - mse: 175.1814 - val_loss: 1.8193 - val_mse: 180.5734\n",
            "Epoch 88/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.7790 - mse: 174.3309 - val_loss: 1.8188 - val_mse: 183.2185\n",
            "Epoch 89/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.7748 - mse: 173.2030 - val_loss: 1.8145 - val_mse: 180.7994\n",
            "Epoch 90/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.7950 - mse: 178.0310 - val_loss: 1.8136 - val_mse: 175.6729\n",
            "Epoch 91/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7823 - mse: 173.7319 - val_loss: 1.8161 - val_mse: 169.4444\n",
            "Epoch 92/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.7824 - mse: 175.0971 - val_loss: 1.8207 - val_mse: 191.7681\n",
            "Epoch 93/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.7889 - mse: 176.4453 - val_loss: 1.8195 - val_mse: 172.5588\n",
            "Epoch 94/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.7805 - mse: 173.4668 - val_loss: 1.8137 - val_mse: 179.2801\n",
            "Epoch 95/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7744 - mse: 173.2813 - val_loss: 1.8085 - val_mse: 178.9890\n",
            "Epoch 96/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7811 - mse: 174.2438 - val_loss: 1.8099 - val_mse: 183.1293\n",
            "Epoch 97/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.7745 - mse: 173.8432 - val_loss: 1.8060 - val_mse: 180.6700\n",
            "Epoch 98/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7724 - mse: 175.3076 - val_loss: 1.8058 - val_mse: 175.9218\n",
            "Epoch 99/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.7654 - mse: 173.1073 - val_loss: 1.8040 - val_mse: 175.2933\n",
            "Epoch 100/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.7765 - mse: 175.1284 - val_loss: 1.8047 - val_mse: 174.1189\n",
            "Epoch 101/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.7618 - mse: 171.4951 - val_loss: 1.8077 - val_mse: 172.9560\n",
            "Epoch 102/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.7640 - mse: 173.0306 - val_loss: 1.8061 - val_mse: 179.9227\n",
            "Epoch 103/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.7743 - mse: 173.7071 - val_loss: 1.8067 - val_mse: 174.3516\n",
            "Epoch 104/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.7637 - mse: 173.1018 - val_loss: 1.8009 - val_mse: 174.4042\n",
            "Epoch 105/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7669 - mse: 174.2532 - val_loss: 1.8004 - val_mse: 174.0795\n",
            "Epoch 106/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7615 - mse: 172.1593 - val_loss: 1.7989 - val_mse: 175.5858\n",
            "Epoch 107/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7663 - mse: 173.3806 - val_loss: 1.7995 - val_mse: 174.8706\n",
            "Epoch 108/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7627 - mse: 172.8366 - val_loss: 1.7993 - val_mse: 175.6148\n",
            "Epoch 109/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7661 - mse: 174.0796 - val_loss: 1.7989 - val_mse: 178.0264\n",
            "Epoch 110/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7581 - mse: 172.0829 - val_loss: 1.7994 - val_mse: 176.3771\n",
            "Epoch 111/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7583 - mse: 171.7319 - val_loss: 1.7980 - val_mse: 175.8630\n",
            "Epoch 112/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.7718 - mse: 174.0123 - val_loss: 1.7978 - val_mse: 175.3270\n",
            "Epoch 113/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.7575 - mse: 171.6359 - val_loss: 1.7977 - val_mse: 174.8786\n",
            "Epoch 114/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.7484 - mse: 170.9423 - val_loss: 1.7977 - val_mse: 175.2815\n",
            "Epoch 115/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.7574 - mse: 171.9279 - val_loss: 1.7974 - val_mse: 177.7989\n",
            "Epoch 116/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.7677 - mse: 174.1935 - val_loss: 1.7975 - val_mse: 180.3191\n",
            "Epoch 117/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7607 - mse: 172.2443 - val_loss: 1.7973 - val_mse: 176.4582\n",
            "Epoch 118/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7567 - mse: 171.5041 - val_loss: 1.7970 - val_mse: 174.7747\n",
            "Epoch 119/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.7588 - mse: 171.3496 - val_loss: 1.7971 - val_mse: 176.4654\n",
            "Epoch 120/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.7594 - mse: 172.7675 - val_loss: 1.7967 - val_mse: 175.9084\n",
            "Epoch 121/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7552 - mse: 171.8219 - val_loss: 1.7971 - val_mse: 177.5964\n",
            "Epoch 122/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7565 - mse: 173.8002 - val_loss: 1.7966 - val_mse: 178.4029\n",
            "Epoch 123/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7539 - mse: 171.8667 - val_loss: 1.7969 - val_mse: 177.6355\n",
            "Epoch 124/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7585 - mse: 172.9000 - val_loss: 1.7971 - val_mse: 181.0837\n",
            "Epoch 125/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.7605 - mse: 173.4990 - val_loss: 1.7959 - val_mse: 177.0421\n",
            "Epoch 126/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.7535 - mse: 172.2300 - val_loss: 1.7959 - val_mse: 177.8560\n",
            "Epoch 127/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7639 - mse: 173.7634 - val_loss: 1.7958 - val_mse: 178.1061\n",
            "Epoch 128/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.7569 - mse: 171.7878 - val_loss: 1.7961 - val_mse: 177.6226\n",
            "Epoch 129/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.7651 - mse: 174.6751 - val_loss: 1.7958 - val_mse: 176.4619\n",
            "Epoch 130/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.7638 - mse: 173.6064 - val_loss: 1.7959 - val_mse: 176.3293\n",
            "Epoch 131/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7566 - mse: 172.1482 - val_loss: 1.7961 - val_mse: 175.1130\n",
            "Epoch 132/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.7513 - mse: 171.6997 - val_loss: 1.7957 - val_mse: 176.8951\n",
            "Epoch 133/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.7589 - mse: 173.0280 - val_loss: 1.7957 - val_mse: 177.1578\n",
            "Epoch 134/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.7545 - mse: 172.0739 - val_loss: 1.7956 - val_mse: 176.8864\n",
            "Epoch 135/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.7539 - mse: 172.4836 - val_loss: 1.7955 - val_mse: 177.9896\n",
            "Epoch 136/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.7525 - mse: 172.7036 - val_loss: 1.7958 - val_mse: 175.6801\n",
            "Epoch 137/500\n",
            "560/560 [==============================] - 14s 24ms/step - loss: 1.7577 - mse: 172.0132 - val_loss: 1.7959 - val_mse: 174.9989\n",
            "Epoch 138/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.7483 - mse: 171.0275 - val_loss: 1.7957 - val_mse: 175.0534\n",
            "Epoch 139/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.7512 - mse: 171.5153 - val_loss: 1.7954 - val_mse: 176.3670\n",
            "Epoch 140/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.7573 - mse: 172.4168 - val_loss: 1.7955 - val_mse: 177.1102\n",
            "Epoch 141/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.7606 - mse: 173.6323 - val_loss: 1.7955 - val_mse: 176.0582\n",
            "Epoch 142/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.7533 - mse: 171.2906 - val_loss: 1.7956 - val_mse: 175.9155\n",
            "Epoch 143/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.7639 - mse: 173.1871 - val_loss: 1.7954 - val_mse: 176.1689\n",
            "Epoch 144/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.7555 - mse: 171.3691 - val_loss: 1.7955 - val_mse: 176.1828\n",
            "Epoch 145/500\n",
            "560/560 [==============================] - 14s 24ms/step - loss: 1.7639 - mse: 173.7509 - val_loss: 1.7953 - val_mse: 176.7215\n",
            "Epoch 146/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.7602 - mse: 172.5854 - val_loss: 1.7954 - val_mse: 177.0305\n",
            "Epoch 147/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.7562 - mse: 173.5889 - val_loss: 1.7953 - val_mse: 176.8362\n",
            "Epoch 148/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.7614 - mse: 173.4170 - val_loss: 1.7953 - val_mse: 177.2352\n",
            "Epoch 149/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.7546 - mse: 172.6849 - val_loss: 1.7954 - val_mse: 176.2676\n",
            "Epoch 150/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.7524 - mse: 171.6670 - val_loss: 1.7953 - val_mse: 177.0212\n",
            "Epoch 151/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.7499 - mse: 171.7391 - val_loss: 1.7954 - val_mse: 176.5670\n",
            "Epoch 152/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.7578 - mse: 173.2260 - val_loss: 1.7954 - val_mse: 176.5422\n",
            "Epoch 153/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.7595 - mse: 173.4873 - val_loss: 1.7954 - val_mse: 176.2866\n",
            "Epoch 154/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.7550 - mse: 171.6662 - val_loss: 1.7954 - val_mse: 176.4749\n",
            "Epoch 155/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.7534 - mse: 172.3314 - val_loss: 1.7954 - val_mse: 176.4238\n",
            "Epoch 156/500\n",
            "560/560 [==============================] - 14s 24ms/step - loss: 1.7498 - mse: 170.7169 - val_loss: 1.7953 - val_mse: 176.5145\n",
            "219/219 [==============================] - 2s 7ms/step - loss: 1.7813 - mse: 171.8602\n",
            "loss:  1.7813189029693604\n",
            "mae:  171.86019897460938\n",
            "0.2번째 지정\n",
            "0.3번째 훈련\n",
            "Epoch 1/500\n",
            "560/560 [==============================] - 14s 23ms/step - loss: 4.7010 - mse: 718.8689 - val_loss: 3.0805 - val_mse: 179.8269\n",
            "Epoch 2/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 3.0371 - mse: 181.7485 - val_loss: 2.8979 - val_mse: 161.7385\n",
            "Epoch 3/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.8878 - mse: 171.9518 - val_loss: 2.8596 - val_mse: 183.3184\n",
            "Epoch 4/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.8100 - mse: 164.2027 - val_loss: 2.7822 - val_mse: 151.6465\n",
            "Epoch 5/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.7750 - mse: 160.6485 - val_loss: 2.7486 - val_mse: 156.6667\n",
            "Epoch 6/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.7681 - mse: 162.0786 - val_loss: 2.7247 - val_mse: 153.5161\n",
            "Epoch 7/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.7390 - mse: 158.6038 - val_loss: 2.7057 - val_mse: 148.4302\n",
            "Epoch 8/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.7131 - mse: 156.6191 - val_loss: 2.6907 - val_mse: 154.7583\n",
            "Epoch 9/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.6960 - mse: 154.9011 - val_loss: 2.6771 - val_mse: 145.4210\n",
            "Epoch 10/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.6706 - mse: 152.9045 - val_loss: 2.6941 - val_mse: 141.2273\n",
            "Epoch 11/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.6540 - mse: 150.2964 - val_loss: 2.6342 - val_mse: 160.2552\n",
            "Epoch 12/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.6128 - mse: 148.2781 - val_loss: 2.6272 - val_mse: 163.4980\n",
            "Epoch 13/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.6267 - mse: 149.9331 - val_loss: 2.5969 - val_mse: 149.0290\n",
            "Epoch 14/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.6043 - mse: 147.6649 - val_loss: 2.5934 - val_mse: 137.1639\n",
            "Epoch 15/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.5927 - mse: 146.5257 - val_loss: 2.5885 - val_mse: 140.7765\n",
            "Epoch 16/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.5541 - mse: 142.0404 - val_loss: 2.5678 - val_mse: 152.1819\n",
            "Epoch 17/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.5430 - mse: 141.5819 - val_loss: 2.5385 - val_mse: 142.2601\n",
            "Epoch 18/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.5384 - mse: 141.1125 - val_loss: 2.5241 - val_mse: 137.2280\n",
            "Epoch 19/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.5256 - mse: 139.2739 - val_loss: 2.5798 - val_mse: 165.5411\n",
            "Epoch 20/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.5176 - mse: 140.7426 - val_loss: 2.5056 - val_mse: 135.2089\n",
            "Epoch 21/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.4978 - mse: 137.6387 - val_loss: 2.5053 - val_mse: 132.5591\n",
            "Epoch 22/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.4895 - mse: 136.6638 - val_loss: 2.5371 - val_mse: 125.7075\n",
            "Epoch 23/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.4990 - mse: 137.5823 - val_loss: 2.5215 - val_mse: 128.2171\n",
            "Epoch 24/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4881 - mse: 136.9449 - val_loss: 2.4997 - val_mse: 126.5202\n",
            "Epoch 25/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4572 - mse: 133.9329 - val_loss: 2.4743 - val_mse: 129.8606\n",
            "Epoch 26/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4651 - mse: 135.2898 - val_loss: 2.4577 - val_mse: 135.2544\n",
            "Epoch 27/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4437 - mse: 133.3567 - val_loss: 2.4972 - val_mse: 127.9827\n",
            "Epoch 28/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4344 - mse: 132.5990 - val_loss: 2.4506 - val_mse: 125.8764\n",
            "Epoch 29/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4317 - mse: 133.1928 - val_loss: 2.5986 - val_mse: 119.7427\n",
            "Epoch 30/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.4160 - mse: 130.8191 - val_loss: 2.4233 - val_mse: 133.9065\n",
            "Epoch 31/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.4100 - mse: 132.4030 - val_loss: 2.4177 - val_mse: 129.1270\n",
            "Epoch 32/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.4120 - mse: 132.1661 - val_loss: 2.4170 - val_mse: 125.4010\n",
            "Epoch 33/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3992 - mse: 129.7243 - val_loss: 2.4160 - val_mse: 122.7831\n",
            "Epoch 34/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3752 - mse: 128.0718 - val_loss: 2.3864 - val_mse: 133.4758\n",
            "Epoch 35/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3714 - mse: 128.4475 - val_loss: 2.3759 - val_mse: 131.4568\n",
            "Epoch 36/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3646 - mse: 128.6304 - val_loss: 2.3811 - val_mse: 130.2463\n",
            "Epoch 37/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3677 - mse: 128.9690 - val_loss: 2.3843 - val_mse: 130.1863\n",
            "Epoch 38/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3571 - mse: 127.3629 - val_loss: 2.3633 - val_mse: 131.2037\n",
            "Epoch 39/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.3437 - mse: 126.5408 - val_loss: 2.3775 - val_mse: 120.2953\n",
            "Epoch 40/500\n",
            "560/560 [==============================] - 14s 24ms/step - loss: 2.3462 - mse: 126.5093 - val_loss: 2.3567 - val_mse: 123.6588\n",
            "Epoch 41/500\n",
            "560/560 [==============================] - 16s 28ms/step - loss: 2.3333 - mse: 126.3029 - val_loss: 2.3510 - val_mse: 132.9636\n",
            "Epoch 42/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 2.3256 - mse: 126.3741 - val_loss: 2.3222 - val_mse: 129.3165\n",
            "Epoch 43/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3139 - mse: 124.9158 - val_loss: 2.3729 - val_mse: 141.2391\n",
            "Epoch 44/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.2986 - mse: 123.1088 - val_loss: 2.3274 - val_mse: 121.3592\n",
            "Epoch 45/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3008 - mse: 124.5020 - val_loss: 2.3204 - val_mse: 117.8834\n",
            "Epoch 46/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.2948 - mse: 123.6415 - val_loss: 2.3281 - val_mse: 131.6505\n",
            "Epoch 47/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.2756 - mse: 122.2539 - val_loss: 2.3247 - val_mse: 119.2708\n",
            "Epoch 48/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2905 - mse: 124.3017 - val_loss: 2.3311 - val_mse: 122.7746\n",
            "Epoch 49/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.2847 - mse: 123.9721 - val_loss: 2.3381 - val_mse: 136.1005\n",
            "Epoch 50/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.2428 - mse: 121.4872 - val_loss: 2.2538 - val_mse: 125.6996\n",
            "Epoch 51/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.2288 - mse: 120.4686 - val_loss: 2.2583 - val_mse: 120.8892\n",
            "Epoch 52/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.2259 - mse: 120.5325 - val_loss: 2.2477 - val_mse: 121.0813\n",
            "Epoch 53/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2214 - mse: 120.3093 - val_loss: 2.2679 - val_mse: 130.4180\n",
            "Epoch 54/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 2.2264 - mse: 121.0749 - val_loss: 2.2519 - val_mse: 120.5860\n",
            "Epoch 55/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2140 - mse: 119.9162 - val_loss: 2.2567 - val_mse: 131.8947\n",
            "Epoch 56/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2140 - mse: 120.6264 - val_loss: 2.2594 - val_mse: 135.9432\n",
            "Epoch 57/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1992 - mse: 120.3496 - val_loss: 2.2160 - val_mse: 123.3367\n",
            "Epoch 58/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1942 - mse: 119.6589 - val_loss: 2.2156 - val_mse: 120.8265\n",
            "Epoch 59/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1891 - mse: 119.1887 - val_loss: 2.2203 - val_mse: 117.9563\n",
            "Epoch 60/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1953 - mse: 119.5285 - val_loss: 2.2118 - val_mse: 124.2109\n",
            "Epoch 61/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1771 - mse: 118.2149 - val_loss: 2.2101 - val_mse: 119.2670\n",
            "Epoch 62/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1802 - mse: 118.5163 - val_loss: 2.2145 - val_mse: 121.1885\n",
            "Epoch 63/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1831 - mse: 119.3347 - val_loss: 2.2250 - val_mse: 115.8136\n",
            "Epoch 64/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1812 - mse: 118.9241 - val_loss: 2.2145 - val_mse: 116.2494\n",
            "Epoch 65/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1772 - mse: 118.6586 - val_loss: 2.2045 - val_mse: 122.1741\n",
            "Epoch 66/500\n",
            "560/560 [==============================] - 14s 24ms/step - loss: 2.1744 - mse: 118.7516 - val_loss: 2.2064 - val_mse: 116.3013\n",
            "Epoch 67/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 2.1751 - mse: 118.3043 - val_loss: 2.2207 - val_mse: 116.2719\n",
            "Epoch 68/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1741 - mse: 118.7708 - val_loss: 2.2041 - val_mse: 121.2253\n",
            "Epoch 69/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1649 - mse: 117.6787 - val_loss: 2.1989 - val_mse: 119.8229\n",
            "Epoch 70/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1699 - mse: 117.3869 - val_loss: 2.1981 - val_mse: 118.9285\n",
            "Epoch 71/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1701 - mse: 119.1002 - val_loss: 2.1981 - val_mse: 120.0210\n",
            "Epoch 72/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.1645 - mse: 117.6718 - val_loss: 2.1984 - val_mse: 121.2798\n",
            "Epoch 73/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.1616 - mse: 118.0158 - val_loss: 2.2068 - val_mse: 125.7952\n",
            "Epoch 74/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.1691 - mse: 119.2336 - val_loss: 2.2056 - val_mse: 121.9403\n",
            "Epoch 75/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.1589 - mse: 118.4679 - val_loss: 2.1887 - val_mse: 120.7749\n",
            "Epoch 76/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.1440 - mse: 116.4727 - val_loss: 2.1878 - val_mse: 122.7264\n",
            "Epoch 77/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1424 - mse: 116.9959 - val_loss: 2.1889 - val_mse: 119.5429\n",
            "Epoch 78/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.1428 - mse: 117.2576 - val_loss: 2.1859 - val_mse: 117.1777\n",
            "Epoch 79/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.1538 - mse: 118.3160 - val_loss: 2.1886 - val_mse: 125.1920\n",
            "Epoch 80/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.1530 - mse: 118.2032 - val_loss: 2.1840 - val_mse: 120.5027\n",
            "Epoch 81/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1493 - mse: 117.6652 - val_loss: 2.1829 - val_mse: 120.5571\n",
            "Epoch 82/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1404 - mse: 116.6340 - val_loss: 2.1822 - val_mse: 122.3879\n",
            "Epoch 83/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1364 - mse: 117.1578 - val_loss: 2.1805 - val_mse: 120.6169\n",
            "Epoch 84/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1509 - mse: 118.2765 - val_loss: 2.1788 - val_mse: 118.2608\n",
            "Epoch 85/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1356 - mse: 116.6709 - val_loss: 2.1798 - val_mse: 121.3881\n",
            "Epoch 86/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1540 - mse: 118.1243 - val_loss: 2.1767 - val_mse: 120.5664\n",
            "Epoch 87/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1387 - mse: 116.7963 - val_loss: 2.1796 - val_mse: 120.6881\n",
            "Epoch 88/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.1490 - mse: 117.6554 - val_loss: 2.1766 - val_mse: 120.2519\n",
            "Epoch 89/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.1486 - mse: 118.2335 - val_loss: 2.1770 - val_mse: 119.2264\n",
            "Epoch 90/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.1394 - mse: 117.0204 - val_loss: 2.1798 - val_mse: 121.5238\n",
            "Epoch 91/500\n",
            "560/560 [==============================] - 14s 24ms/step - loss: 2.1428 - mse: 117.5432 - val_loss: 2.1755 - val_mse: 119.0457\n",
            "Epoch 92/500\n",
            "560/560 [==============================] - 15s 27ms/step - loss: 2.1226 - mse: 115.6417 - val_loss: 2.1806 - val_mse: 123.8142\n",
            "Epoch 93/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.1555 - mse: 119.3279 - val_loss: 2.1805 - val_mse: 122.4496\n",
            "Epoch 94/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1425 - mse: 117.7576 - val_loss: 2.1743 - val_mse: 121.2852\n",
            "Epoch 95/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1399 - mse: 117.8559 - val_loss: 2.1804 - val_mse: 115.0294\n",
            "Epoch 96/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.1459 - mse: 117.3571 - val_loss: 2.1749 - val_mse: 118.0163\n",
            "Epoch 97/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1415 - mse: 117.9329 - val_loss: 2.1717 - val_mse: 117.5856\n",
            "Epoch 98/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1399 - mse: 117.7873 - val_loss: 2.1732 - val_mse: 120.2949\n",
            "Epoch 99/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.1452 - mse: 118.5367 - val_loss: 2.1726 - val_mse: 118.0187\n",
            "Epoch 100/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1436 - mse: 116.9233 - val_loss: 2.1762 - val_mse: 116.0450\n",
            "Epoch 101/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1448 - mse: 117.8898 - val_loss: 2.1727 - val_mse: 118.3339\n",
            "Epoch 102/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1273 - mse: 117.0842 - val_loss: 2.1666 - val_mse: 119.1172\n",
            "Epoch 103/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.1380 - mse: 116.9203 - val_loss: 2.1661 - val_mse: 119.1619\n",
            "Epoch 104/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.1463 - mse: 117.4774 - val_loss: 2.1644 - val_mse: 118.4230\n",
            "Epoch 105/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1260 - mse: 116.1463 - val_loss: 2.1656 - val_mse: 119.7810\n",
            "Epoch 106/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1357 - mse: 117.3350 - val_loss: 2.1654 - val_mse: 117.8742\n",
            "Epoch 107/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1289 - mse: 117.1258 - val_loss: 2.1644 - val_mse: 118.7088\n",
            "Epoch 108/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1303 - mse: 116.7865 - val_loss: 2.1643 - val_mse: 120.0514\n",
            "Epoch 109/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1211 - mse: 116.2206 - val_loss: 2.1627 - val_mse: 118.9095\n",
            "Epoch 110/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1344 - mse: 117.8740 - val_loss: 2.1633 - val_mse: 119.5419\n",
            "Epoch 111/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1131 - mse: 115.8658 - val_loss: 2.1623 - val_mse: 119.4680\n",
            "Epoch 112/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1359 - mse: 117.3723 - val_loss: 2.1627 - val_mse: 118.3874\n",
            "Epoch 113/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1346 - mse: 117.6399 - val_loss: 2.1640 - val_mse: 117.4654\n",
            "Epoch 114/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1404 - mse: 117.8592 - val_loss: 2.1640 - val_mse: 117.2602\n",
            "Epoch 115/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.1246 - mse: 116.9087 - val_loss: 2.1625 - val_mse: 118.9874\n",
            "Epoch 116/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.1304 - mse: 117.0416 - val_loss: 2.1611 - val_mse: 119.5357\n",
            "Epoch 117/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.1281 - mse: 117.2288 - val_loss: 2.1614 - val_mse: 118.5676\n",
            "Epoch 118/500\n",
            "560/560 [==============================] - 14s 24ms/step - loss: 2.1334 - mse: 117.8761 - val_loss: 2.1609 - val_mse: 119.1758\n",
            "Epoch 119/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 2.1371 - mse: 117.6432 - val_loss: 2.1612 - val_mse: 118.2663\n",
            "Epoch 120/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1275 - mse: 116.8253 - val_loss: 2.1612 - val_mse: 118.1099\n",
            "Epoch 121/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1219 - mse: 116.4893 - val_loss: 2.1617 - val_mse: 118.1816\n",
            "Epoch 122/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1195 - mse: 116.0550 - val_loss: 2.1610 - val_mse: 119.2344\n",
            "Epoch 123/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.1185 - mse: 117.1338 - val_loss: 2.1610 - val_mse: 118.2273\n",
            "Epoch 124/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1244 - mse: 116.5719 - val_loss: 2.1606 - val_mse: 118.7152\n",
            "Epoch 125/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.1296 - mse: 117.2862 - val_loss: 2.1604 - val_mse: 119.0273\n",
            "Epoch 126/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1216 - mse: 116.4321 - val_loss: 2.1605 - val_mse: 118.7507\n",
            "Epoch 127/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.1205 - mse: 116.1219 - val_loss: 2.1604 - val_mse: 118.9103\n",
            "Epoch 128/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.1209 - mse: 116.9695 - val_loss: 2.1606 - val_mse: 118.6462\n",
            "Epoch 129/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.1149 - mse: 115.5316 - val_loss: 2.1608 - val_mse: 118.4601\n",
            "Epoch 130/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.1246 - mse: 116.7830 - val_loss: 2.1601 - val_mse: 118.9243\n",
            "Epoch 131/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1304 - mse: 116.7875 - val_loss: 2.1601 - val_mse: 119.0472\n",
            "Epoch 132/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.1159 - mse: 115.6786 - val_loss: 2.1600 - val_mse: 118.7930\n",
            "Epoch 133/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.1355 - mse: 118.1077 - val_loss: 2.1601 - val_mse: 118.7216\n",
            "Epoch 134/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.1222 - mse: 116.7165 - val_loss: 2.1601 - val_mse: 118.7976\n",
            "Epoch 135/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.1292 - mse: 117.5943 - val_loss: 2.1600 - val_mse: 119.2383\n",
            "Epoch 136/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.1208 - mse: 116.7114 - val_loss: 2.1600 - val_mse: 119.1180\n",
            "Epoch 137/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1128 - mse: 116.3141 - val_loss: 2.1601 - val_mse: 119.0008\n",
            "Epoch 138/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1147 - mse: 116.1642 - val_loss: 2.1600 - val_mse: 119.1755\n",
            "Epoch 139/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.1213 - mse: 116.9908 - val_loss: 2.1600 - val_mse: 118.9750\n",
            "Epoch 140/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.1275 - mse: 117.8599 - val_loss: 2.1600 - val_mse: 118.8917\n",
            "Epoch 141/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1178 - mse: 116.4836 - val_loss: 2.1600 - val_mse: 118.9655\n",
            "Epoch 142/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1284 - mse: 117.5405 - val_loss: 2.1600 - val_mse: 118.8340\n",
            "Epoch 143/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1222 - mse: 116.4838 - val_loss: 2.1600 - val_mse: 118.9542\n",
            "Epoch 144/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.1343 - mse: 117.6702 - val_loss: 2.1600 - val_mse: 118.9473\n",
            "Epoch 145/500\n",
            "560/560 [==============================] - 14s 24ms/step - loss: 2.1145 - mse: 116.3718 - val_loss: 2.1600 - val_mse: 118.8611\n",
            "Epoch 146/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1248 - mse: 116.5077 - val_loss: 2.1600 - val_mse: 118.8719\n",
            "Epoch 147/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1229 - mse: 117.0423 - val_loss: 2.1599 - val_mse: 119.0962\n",
            "Epoch 148/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.1234 - mse: 117.2626 - val_loss: 2.1599 - val_mse: 119.0002\n",
            "Epoch 149/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1310 - mse: 117.4833 - val_loss: 2.1599 - val_mse: 118.9818\n",
            "Epoch 150/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1174 - mse: 116.4335 - val_loss: 2.1599 - val_mse: 119.0018\n",
            "Epoch 151/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1284 - mse: 117.4978 - val_loss: 2.1600 - val_mse: 118.9550\n",
            "Epoch 152/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1215 - mse: 116.9484 - val_loss: 2.1599 - val_mse: 119.0093\n",
            "Epoch 153/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1255 - mse: 117.1122 - val_loss: 2.1600 - val_mse: 118.9861\n",
            "Epoch 154/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 2.1196 - mse: 116.3719 - val_loss: 2.1600 - val_mse: 118.9853\n",
            "Epoch 155/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1267 - mse: 117.2233 - val_loss: 2.1600 - val_mse: 118.9876\n",
            "Epoch 156/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.1189 - mse: 116.4506 - val_loss: 2.1599 - val_mse: 118.9906\n",
            "219/219 [==============================] - 1s 6ms/step - loss: 2.1376 - mse: 115.0972\n",
            "loss:  2.1375999450683594\n",
            "mae:  115.09716033935547\n",
            "0.3번째 지정\n",
            "0.4번째 훈련\n",
            "Epoch 1/500\n",
            "560/560 [==============================] - 14s 22ms/step - loss: 5.4357 - mse: 521.6742 - val_loss: 3.4832 - val_mse: 144.9701\n",
            "Epoch 2/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 3.3847 - mse: 145.7777 - val_loss: 3.1353 - val_mse: 140.6933\n",
            "Epoch 3/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 3.0999 - mse: 140.3182 - val_loss: 3.0290 - val_mse: 136.4538\n",
            "Epoch 4/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 3.0466 - mse: 137.7436 - val_loss: 3.0186 - val_mse: 136.6270\n",
            "Epoch 5/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 3.0095 - mse: 136.2316 - val_loss: 2.9720 - val_mse: 136.1324\n",
            "Epoch 6/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.9762 - mse: 134.7722 - val_loss: 2.9639 - val_mse: 134.3070\n",
            "Epoch 7/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.9574 - mse: 133.6975 - val_loss: 2.9282 - val_mse: 131.1781\n",
            "Epoch 8/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.9233 - mse: 131.6296 - val_loss: 2.8969 - val_mse: 126.8899\n",
            "Epoch 9/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.8948 - mse: 129.7634 - val_loss: 2.9003 - val_mse: 129.3917\n",
            "Epoch 10/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.8775 - mse: 129.2144 - val_loss: 2.8729 - val_mse: 131.2034\n",
            "Epoch 11/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.8540 - mse: 127.9532 - val_loss: 2.8355 - val_mse: 124.7406\n",
            "Epoch 12/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.8400 - mse: 126.7558 - val_loss: 2.8867 - val_mse: 131.6685\n",
            "Epoch 13/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.8536 - mse: 129.6522 - val_loss: 2.8853 - val_mse: 130.4394\n",
            "Epoch 14/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.8467 - mse: 127.6255 - val_loss: 2.7820 - val_mse: 121.1977\n",
            "Epoch 15/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.7902 - mse: 123.8348 - val_loss: 2.7832 - val_mse: 122.7474\n",
            "Epoch 16/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.7690 - mse: 122.2917 - val_loss: 2.9020 - val_mse: 125.8444\n",
            "Epoch 17/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.7658 - mse: 122.0871 - val_loss: 2.7580 - val_mse: 122.6012\n",
            "Epoch 18/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.7446 - mse: 121.2981 - val_loss: 2.7503 - val_mse: 120.7362\n",
            "Epoch 19/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.7219 - mse: 119.5896 - val_loss: 2.7957 - val_mse: 123.6721\n",
            "Epoch 20/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.7265 - mse: 119.9653 - val_loss: 2.7763 - val_mse: 124.6774\n",
            "Epoch 21/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.7260 - mse: 120.1276 - val_loss: 2.6892 - val_mse: 118.4029\n",
            "Epoch 22/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.7115 - mse: 119.5167 - val_loss: 2.6936 - val_mse: 119.8229\n",
            "Epoch 23/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.6779 - mse: 117.4275 - val_loss: 2.7323 - val_mse: 122.6014\n",
            "Epoch 24/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.6721 - mse: 116.7386 - val_loss: 2.6957 - val_mse: 120.2739\n",
            "Epoch 25/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.6799 - mse: 117.9382 - val_loss: 2.6648 - val_mse: 115.8020\n",
            "Epoch 26/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.6648 - mse: 116.6639 - val_loss: 2.6396 - val_mse: 114.1417\n",
            "Epoch 27/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.6498 - mse: 116.1339 - val_loss: 2.6385 - val_mse: 115.0251\n",
            "Epoch 28/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.6313 - mse: 115.1609 - val_loss: 2.6278 - val_mse: 115.4090\n",
            "Epoch 29/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.6211 - mse: 114.8757 - val_loss: 2.6564 - val_mse: 113.9737\n",
            "Epoch 30/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.6242 - mse: 114.8265 - val_loss: 2.6049 - val_mse: 112.9311\n",
            "Epoch 31/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.6237 - mse: 114.9146 - val_loss: 2.6102 - val_mse: 112.5633\n",
            "Epoch 32/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.6235 - mse: 114.9571 - val_loss: 2.7023 - val_mse: 119.6163\n",
            "Epoch 33/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.5968 - mse: 113.4415 - val_loss: 2.5688 - val_mse: 110.7478\n",
            "Epoch 34/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.5722 - mse: 111.8336 - val_loss: 2.5533 - val_mse: 110.5918\n",
            "Epoch 35/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.5633 - mse: 111.2166 - val_loss: 2.5822 - val_mse: 111.5388\n",
            "Epoch 36/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.5607 - mse: 111.3710 - val_loss: 2.5829 - val_mse: 112.0491\n",
            "Epoch 37/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.5479 - mse: 110.9799 - val_loss: 2.5908 - val_mse: 113.2328\n",
            "Epoch 38/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.5538 - mse: 111.0896 - val_loss: 2.5669 - val_mse: 111.0277\n",
            "Epoch 39/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.5017 - mse: 109.4773 - val_loss: 2.4869 - val_mse: 108.1638\n",
            "Epoch 40/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.4907 - mse: 109.1198 - val_loss: 2.4963 - val_mse: 108.7850\n",
            "Epoch 41/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 2.4938 - mse: 109.1986 - val_loss: 2.4923 - val_mse: 108.1785\n",
            "Epoch 42/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.4730 - mse: 107.6337 - val_loss: 2.4848 - val_mse: 107.4149\n",
            "Epoch 43/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.4732 - mse: 107.3456 - val_loss: 2.4857 - val_mse: 106.9426\n",
            "Epoch 44/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.4721 - mse: 107.7066 - val_loss: 2.4930 - val_mse: 109.3959\n",
            "Epoch 45/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.4688 - mse: 107.6621 - val_loss: 2.4696 - val_mse: 107.2310\n",
            "Epoch 46/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.4580 - mse: 107.4263 - val_loss: 2.4643 - val_mse: 107.1381\n",
            "Epoch 47/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.4561 - mse: 107.4657 - val_loss: 2.4651 - val_mse: 106.0387\n",
            "Epoch 48/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.4663 - mse: 107.5758 - val_loss: 2.4808 - val_mse: 108.3335\n",
            "Epoch 49/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.4577 - mse: 107.3483 - val_loss: 2.4887 - val_mse: 111.0095\n",
            "Epoch 50/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.4558 - mse: 106.9041 - val_loss: 2.4503 - val_mse: 106.1506\n",
            "Epoch 51/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.4385 - mse: 106.1904 - val_loss: 2.4404 - val_mse: 107.0405\n",
            "Epoch 52/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.4348 - mse: 106.0487 - val_loss: 2.4477 - val_mse: 106.7230\n",
            "Epoch 53/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.4525 - mse: 106.6259 - val_loss: 2.4403 - val_mse: 106.7974\n",
            "Epoch 54/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.4386 - mse: 106.3104 - val_loss: 2.4511 - val_mse: 106.2549\n",
            "Epoch 55/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4206 - mse: 105.0452 - val_loss: 2.4464 - val_mse: 106.7676\n",
            "Epoch 56/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.4000 - mse: 104.6782 - val_loss: 2.4210 - val_mse: 105.0210\n",
            "Epoch 57/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3894 - mse: 104.1421 - val_loss: 2.4353 - val_mse: 105.4773\n",
            "Epoch 58/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.4064 - mse: 104.8652 - val_loss: 2.4311 - val_mse: 104.9078\n",
            "Epoch 59/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.4138 - mse: 105.3808 - val_loss: 2.4134 - val_mse: 104.7092\n",
            "Epoch 60/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3922 - mse: 103.9436 - val_loss: 2.4296 - val_mse: 105.7983\n",
            "Epoch 61/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3945 - mse: 104.1137 - val_loss: 2.4073 - val_mse: 104.3315\n",
            "Epoch 62/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3986 - mse: 104.6902 - val_loss: 2.4182 - val_mse: 105.0925\n",
            "Epoch 63/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3966 - mse: 104.3106 - val_loss: 2.4130 - val_mse: 104.1712\n",
            "Epoch 64/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3825 - mse: 103.7685 - val_loss: 2.4066 - val_mse: 105.0213\n",
            "Epoch 65/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3895 - mse: 104.0800 - val_loss: 2.4012 - val_mse: 104.4380\n",
            "Epoch 66/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3975 - mse: 105.0066 - val_loss: 2.4007 - val_mse: 104.3532\n",
            "Epoch 67/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.3963 - mse: 104.8319 - val_loss: 2.4187 - val_mse: 105.9492\n",
            "Epoch 68/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3859 - mse: 104.0217 - val_loss: 2.4018 - val_mse: 105.1024\n",
            "Epoch 69/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3872 - mse: 104.2735 - val_loss: 2.3988 - val_mse: 104.5493\n",
            "Epoch 70/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3733 - mse: 103.0843 - val_loss: 2.4131 - val_mse: 105.2556\n",
            "Epoch 71/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3788 - mse: 103.6495 - val_loss: 2.3955 - val_mse: 103.9049\n",
            "Epoch 72/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3701 - mse: 103.4100 - val_loss: 2.4042 - val_mse: 105.8085\n",
            "Epoch 73/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3713 - mse: 103.2779 - val_loss: 2.3978 - val_mse: 104.9447\n",
            "Epoch 74/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3750 - mse: 103.2955 - val_loss: 2.3892 - val_mse: 104.0255\n",
            "Epoch 75/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3636 - mse: 102.6560 - val_loss: 2.3896 - val_mse: 104.0170\n",
            "Epoch 76/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3809 - mse: 104.0451 - val_loss: 2.3864 - val_mse: 103.9330\n",
            "Epoch 77/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3644 - mse: 103.1635 - val_loss: 2.4005 - val_mse: 104.1113\n",
            "Epoch 78/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3588 - mse: 102.5737 - val_loss: 2.3868 - val_mse: 104.4086\n",
            "Epoch 79/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3622 - mse: 102.6194 - val_loss: 2.3945 - val_mse: 104.3522\n",
            "Epoch 80/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3680 - mse: 103.4757 - val_loss: 2.3817 - val_mse: 103.4696\n",
            "Epoch 81/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3691 - mse: 103.2629 - val_loss: 2.3827 - val_mse: 103.9996\n",
            "Epoch 82/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3591 - mse: 103.0469 - val_loss: 2.3798 - val_mse: 104.1183\n",
            "Epoch 83/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3755 - mse: 103.8811 - val_loss: 2.3762 - val_mse: 104.1720\n",
            "Epoch 84/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3632 - mse: 103.1396 - val_loss: 2.3792 - val_mse: 104.5788\n",
            "Epoch 85/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3631 - mse: 103.2791 - val_loss: 2.3777 - val_mse: 103.8956\n",
            "Epoch 86/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3726 - mse: 103.9003 - val_loss: 2.3706 - val_mse: 102.7769\n",
            "Epoch 87/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3599 - mse: 102.8268 - val_loss: 2.3750 - val_mse: 102.9891\n",
            "Epoch 88/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3546 - mse: 102.4406 - val_loss: 2.3735 - val_mse: 103.5859\n",
            "Epoch 89/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3465 - mse: 102.3286 - val_loss: 2.3779 - val_mse: 103.6178\n",
            "Epoch 90/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3420 - mse: 101.6011 - val_loss: 2.3868 - val_mse: 104.7056\n",
            "Epoch 91/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3417 - mse: 102.0403 - val_loss: 2.3620 - val_mse: 103.8352\n",
            "Epoch 92/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3499 - mse: 102.6474 - val_loss: 2.3579 - val_mse: 102.5565\n",
            "Epoch 93/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 2.3460 - mse: 102.1619 - val_loss: 2.3602 - val_mse: 103.2891\n",
            "Epoch 94/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 2.3378 - mse: 102.2462 - val_loss: 2.3643 - val_mse: 103.6430\n",
            "Epoch 95/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3320 - mse: 101.9018 - val_loss: 2.3566 - val_mse: 102.9837\n",
            "Epoch 96/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3428 - mse: 102.4784 - val_loss: 2.3543 - val_mse: 102.9893\n",
            "Epoch 97/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3311 - mse: 101.7546 - val_loss: 2.3611 - val_mse: 102.6819\n",
            "Epoch 98/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3383 - mse: 102.1975 - val_loss: 2.3559 - val_mse: 101.9559\n",
            "Epoch 99/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3428 - mse: 102.2600 - val_loss: 2.3593 - val_mse: 103.2969\n",
            "Epoch 100/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3421 - mse: 102.2216 - val_loss: 2.3674 - val_mse: 103.7625\n",
            "Epoch 101/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3314 - mse: 101.7692 - val_loss: 2.3471 - val_mse: 102.7821\n",
            "Epoch 102/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3230 - mse: 101.3907 - val_loss: 2.3464 - val_mse: 102.6689\n",
            "Epoch 103/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3318 - mse: 101.9131 - val_loss: 2.3476 - val_mse: 102.2789\n",
            "Epoch 104/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3222 - mse: 101.1966 - val_loss: 2.3493 - val_mse: 103.0542\n",
            "Epoch 105/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3164 - mse: 100.9706 - val_loss: 2.3457 - val_mse: 102.7492\n",
            "Epoch 106/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3177 - mse: 100.9933 - val_loss: 2.3479 - val_mse: 102.4371\n",
            "Epoch 107/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3326 - mse: 102.1794 - val_loss: 2.3443 - val_mse: 102.3666\n",
            "Epoch 108/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3290 - mse: 101.9530 - val_loss: 2.3461 - val_mse: 102.6253\n",
            "Epoch 109/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3217 - mse: 101.0810 - val_loss: 2.3460 - val_mse: 102.5458\n",
            "Epoch 110/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3188 - mse: 101.3763 - val_loss: 2.3431 - val_mse: 102.4874\n",
            "Epoch 111/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3196 - mse: 101.1933 - val_loss: 2.3454 - val_mse: 102.6943\n",
            "Epoch 112/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3072 - mse: 100.2109 - val_loss: 2.3429 - val_mse: 102.2944\n",
            "Epoch 113/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3297 - mse: 101.9481 - val_loss: 2.3430 - val_mse: 102.4025\n",
            "Epoch 114/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3147 - mse: 100.5991 - val_loss: 2.3434 - val_mse: 102.1858\n",
            "Epoch 115/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3216 - mse: 101.1210 - val_loss: 2.3439 - val_mse: 102.6562\n",
            "Epoch 116/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3140 - mse: 101.0683 - val_loss: 2.3451 - val_mse: 102.6467\n",
            "Epoch 117/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3278 - mse: 101.6376 - val_loss: 2.3423 - val_mse: 102.6256\n",
            "Epoch 118/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3098 - mse: 100.9841 - val_loss: 2.3410 - val_mse: 101.9866\n",
            "Epoch 119/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3186 - mse: 101.5517 - val_loss: 2.3388 - val_mse: 102.3417\n",
            "Epoch 120/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 2.3140 - mse: 100.8320 - val_loss: 2.3399 - val_mse: 102.3349\n",
            "Epoch 121/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3100 - mse: 100.5798 - val_loss: 2.3408 - val_mse: 102.5240\n",
            "Epoch 122/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3121 - mse: 101.0030 - val_loss: 2.3395 - val_mse: 102.4792\n",
            "Epoch 123/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3279 - mse: 101.8821 - val_loss: 2.3399 - val_mse: 102.2583\n",
            "Epoch 124/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.3267 - mse: 101.9594 - val_loss: 2.3378 - val_mse: 102.2235\n",
            "Epoch 125/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3139 - mse: 101.1452 - val_loss: 2.3383 - val_mse: 102.3599\n",
            "Epoch 126/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3139 - mse: 101.0314 - val_loss: 2.3377 - val_mse: 102.1509\n",
            "Epoch 127/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3168 - mse: 100.9601 - val_loss: 2.3375 - val_mse: 102.3062\n",
            "Epoch 128/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3157 - mse: 100.8671 - val_loss: 2.3378 - val_mse: 102.1988\n",
            "Epoch 129/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3208 - mse: 101.3727 - val_loss: 2.3372 - val_mse: 102.2513\n",
            "Epoch 130/500\n",
            "560/560 [==============================] - 14s 24ms/step - loss: 2.3046 - mse: 100.4384 - val_loss: 2.3372 - val_mse: 102.2640\n",
            "Epoch 131/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3052 - mse: 100.4305 - val_loss: 2.3388 - val_mse: 102.2870\n",
            "Epoch 132/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.3137 - mse: 101.0846 - val_loss: 2.3373 - val_mse: 102.0795\n",
            "Epoch 133/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3161 - mse: 101.4350 - val_loss: 2.3374 - val_mse: 102.1060\n",
            "Epoch 134/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3049 - mse: 100.4054 - val_loss: 2.3365 - val_mse: 102.2563\n",
            "Epoch 135/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3065 - mse: 100.8669 - val_loss: 2.3369 - val_mse: 102.3248\n",
            "Epoch 136/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3049 - mse: 100.5932 - val_loss: 2.3368 - val_mse: 102.1482\n",
            "Epoch 137/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3322 - mse: 102.4561 - val_loss: 2.3367 - val_mse: 102.3196\n",
            "Epoch 138/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3177 - mse: 101.2399 - val_loss: 2.3366 - val_mse: 102.2104\n",
            "Epoch 139/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3168 - mse: 101.3442 - val_loss: 2.3363 - val_mse: 102.2518\n",
            "Epoch 140/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3036 - mse: 100.2743 - val_loss: 2.3366 - val_mse: 102.1497\n",
            "Epoch 141/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3104 - mse: 100.6653 - val_loss: 2.3361 - val_mse: 102.2661\n",
            "Epoch 142/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3071 - mse: 100.7785 - val_loss: 2.3363 - val_mse: 102.2808\n",
            "Epoch 143/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3058 - mse: 100.4288 - val_loss: 2.3361 - val_mse: 102.2791\n",
            "Epoch 144/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3127 - mse: 101.2284 - val_loss: 2.3362 - val_mse: 102.2450\n",
            "Epoch 145/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 2.3205 - mse: 101.8681 - val_loss: 2.3363 - val_mse: 102.1662\n",
            "Epoch 146/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3130 - mse: 100.8423 - val_loss: 2.3362 - val_mse: 102.1703\n",
            "Epoch 147/500\n",
            "560/560 [==============================] - 14s 24ms/step - loss: 2.3244 - mse: 101.9938 - val_loss: 2.3361 - val_mse: 102.2092\n",
            "Epoch 148/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3118 - mse: 100.7638 - val_loss: 2.3361 - val_mse: 102.2110\n",
            "Epoch 149/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3075 - mse: 100.6548 - val_loss: 2.3360 - val_mse: 102.2275\n",
            "Epoch 150/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3078 - mse: 100.8866 - val_loss: 2.3361 - val_mse: 102.2213\n",
            "Epoch 151/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3083 - mse: 100.8338 - val_loss: 2.3360 - val_mse: 102.2258\n",
            "Epoch 152/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3052 - mse: 100.7042 - val_loss: 2.3360 - val_mse: 102.1856\n",
            "Epoch 153/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3207 - mse: 101.5677 - val_loss: 2.3361 - val_mse: 102.1689\n",
            "Epoch 154/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3220 - mse: 101.3881 - val_loss: 2.3360 - val_mse: 102.1977\n",
            "Epoch 155/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3093 - mse: 100.2521 - val_loss: 2.3359 - val_mse: 102.2019\n",
            "Epoch 156/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.3125 - mse: 100.9600 - val_loss: 2.3360 - val_mse: 102.2087\n",
            "Epoch 157/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3042 - mse: 100.5084 - val_loss: 2.3359 - val_mse: 102.2307\n",
            "Epoch 158/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.3099 - mse: 100.9776 - val_loss: 2.3360 - val_mse: 102.2115\n",
            "Epoch 159/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3046 - mse: 100.4711 - val_loss: 2.3360 - val_mse: 102.1994\n",
            "Epoch 160/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3152 - mse: 101.2478 - val_loss: 2.3359 - val_mse: 102.2099\n",
            "Epoch 161/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3061 - mse: 100.5206 - val_loss: 2.3360 - val_mse: 102.2135\n",
            "Epoch 162/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3135 - mse: 101.2765 - val_loss: 2.3359 - val_mse: 102.2124\n",
            "Epoch 163/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3019 - mse: 100.4267 - val_loss: 2.3359 - val_mse: 102.2168\n",
            "Epoch 164/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3193 - mse: 101.5818 - val_loss: 2.3359 - val_mse: 102.2151\n",
            "Epoch 165/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3240 - mse: 101.8069 - val_loss: 2.3359 - val_mse: 102.2093\n",
            "Epoch 166/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3067 - mse: 100.9791 - val_loss: 2.3359 - val_mse: 102.2189\n",
            "Epoch 167/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3065 - mse: 100.6946 - val_loss: 2.3359 - val_mse: 102.2140\n",
            "Epoch 168/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.3039 - mse: 100.6325 - val_loss: 2.3359 - val_mse: 102.2137\n",
            "Epoch 169/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3089 - mse: 100.7019 - val_loss: 2.3359 - val_mse: 102.2124\n",
            "Epoch 170/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 2.3053 - mse: 100.5007 - val_loss: 2.3359 - val_mse: 102.2104\n",
            "Epoch 171/500\n",
            "560/560 [==============================] - 14s 26ms/step - loss: 2.3112 - mse: 100.9120 - val_loss: 2.3359 - val_mse: 102.2134\n",
            "Epoch 172/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3126 - mse: 101.2983 - val_loss: 2.3359 - val_mse: 102.2122\n",
            "Epoch 173/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.3152 - mse: 101.0600 - val_loss: 2.3359 - val_mse: 102.2129\n",
            "Epoch 174/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3146 - mse: 101.4346 - val_loss: 2.3359 - val_mse: 102.2135\n",
            "219/219 [==============================] - 1s 7ms/step - loss: 2.3193 - mse: 100.7752\n",
            "loss:  2.3193278312683105\n",
            "mae:  100.7752456665039\n",
            "0.4번째 지정\n",
            "0.5번째 훈련\n",
            "Epoch 1/500\n",
            "560/560 [==============================] - 14s 23ms/step - loss: 6.5099 - mse: 530.9584 - val_loss: 3.5052 - val_mse: 144.8286\n",
            "Epoch 2/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 3.4206 - mse: 147.2956 - val_loss: 3.2218 - val_mse: 143.0489\n",
            "Epoch 3/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 3.1558 - mse: 144.4245 - val_loss: 3.0007 - val_mse: 139.2915\n",
            "Epoch 4/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 3.0296 - mse: 143.9517 - val_loss: 3.0020 - val_mse: 141.3414\n",
            "Epoch 5/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 3.0141 - mse: 148.6480 - val_loss: 2.9735 - val_mse: 150.9541\n",
            "Epoch 6/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.9341 - mse: 141.2855 - val_loss: 2.8910 - val_mse: 135.0823\n",
            "Epoch 7/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.9034 - mse: 138.8011 - val_loss: 2.8518 - val_mse: 133.7494\n",
            "Epoch 8/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.8858 - mse: 138.7004 - val_loss: 2.8320 - val_mse: 134.3180\n",
            "Epoch 9/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.8317 - mse: 135.7075 - val_loss: 2.9317 - val_mse: 130.3522\n",
            "Epoch 10/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.8412 - mse: 135.5730 - val_loss: 2.7886 - val_mse: 135.3983\n",
            "Epoch 11/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.7811 - mse: 134.1391 - val_loss: 2.7917 - val_mse: 130.1849\n",
            "Epoch 12/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.7890 - mse: 135.1336 - val_loss: 2.7762 - val_mse: 130.7852\n",
            "Epoch 13/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.7929 - mse: 134.7390 - val_loss: 2.7930 - val_mse: 142.7421\n",
            "Epoch 14/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.7614 - mse: 135.1480 - val_loss: 2.7606 - val_mse: 139.8880\n",
            "Epoch 15/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.7545 - mse: 134.2281 - val_loss: 2.7258 - val_mse: 131.8462\n",
            "Epoch 16/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.7305 - mse: 132.7968 - val_loss: 2.7009 - val_mse: 134.8395\n",
            "Epoch 17/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.7051 - mse: 131.7369 - val_loss: 2.7830 - val_mse: 123.7877\n",
            "Epoch 18/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.7251 - mse: 131.6653 - val_loss: 2.7711 - val_mse: 121.3885\n",
            "Epoch 19/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.7278 - mse: 131.8846 - val_loss: 2.6970 - val_mse: 125.7460\n",
            "Epoch 20/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.6796 - mse: 128.7889 - val_loss: 2.6720 - val_mse: 126.8462\n",
            "Epoch 21/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.6798 - mse: 129.1678 - val_loss: 2.7059 - val_mse: 128.3171\n",
            "Epoch 22/500\n",
            "560/560 [==============================] - 14s 24ms/step - loss: 2.6640 - mse: 128.8480 - val_loss: 2.6560 - val_mse: 122.5926\n",
            "Epoch 23/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.6582 - mse: 127.6924 - val_loss: 2.6573 - val_mse: 125.4615\n",
            "Epoch 24/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.6466 - mse: 126.7706 - val_loss: 2.6621 - val_mse: 134.0218\n",
            "Epoch 25/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.6302 - mse: 126.5075 - val_loss: 2.6951 - val_mse: 133.4835\n",
            "Epoch 26/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.6460 - mse: 127.5147 - val_loss: 2.6394 - val_mse: 125.8517\n",
            "Epoch 27/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.6419 - mse: 127.7072 - val_loss: 2.6243 - val_mse: 128.3035\n",
            "Epoch 28/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.6380 - mse: 126.2984 - val_loss: 2.6526 - val_mse: 123.8772\n",
            "Epoch 29/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.6263 - mse: 126.1295 - val_loss: 2.6021 - val_mse: 125.6027\n",
            "Epoch 30/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.6019 - mse: 124.4031 - val_loss: 2.5840 - val_mse: 122.6085\n",
            "Epoch 31/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.6123 - mse: 125.1539 - val_loss: 2.5954 - val_mse: 124.9791\n",
            "Epoch 32/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.5994 - mse: 123.8078 - val_loss: 2.6361 - val_mse: 117.4021\n",
            "Epoch 33/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.5888 - mse: 122.9462 - val_loss: 2.5777 - val_mse: 120.1262\n",
            "Epoch 34/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.5877 - mse: 124.0585 - val_loss: 2.5535 - val_mse: 117.5106\n",
            "Epoch 35/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.5886 - mse: 123.8449 - val_loss: 2.5965 - val_mse: 130.4388\n",
            "Epoch 36/500\n",
            "560/560 [==============================] - 11s 20ms/step - loss: 2.5650 - mse: 122.4235 - val_loss: 2.5541 - val_mse: 118.0546\n",
            "Epoch 37/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.5566 - mse: 122.1006 - val_loss: 2.5554 - val_mse: 125.2439\n",
            "Epoch 38/500\n",
            "560/560 [==============================] - 11s 20ms/step - loss: 2.5422 - mse: 120.8803 - val_loss: 2.6480 - val_mse: 137.1214\n",
            "Epoch 39/500\n",
            "560/560 [==============================] - 11s 21ms/step - loss: 2.5091 - mse: 120.8078 - val_loss: 2.5015 - val_mse: 120.4525\n",
            "Epoch 40/500\n",
            "560/560 [==============================] - 11s 20ms/step - loss: 2.5115 - mse: 121.1656 - val_loss: 2.5217 - val_mse: 115.7789\n",
            "Epoch 41/500\n",
            "560/560 [==============================] - 11s 20ms/step - loss: 2.4924 - mse: 119.5985 - val_loss: 2.4856 - val_mse: 119.6632\n",
            "Epoch 42/500\n",
            "560/560 [==============================] - 11s 21ms/step - loss: 2.4898 - mse: 119.6276 - val_loss: 2.4988 - val_mse: 116.8191\n",
            "Epoch 43/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4961 - mse: 119.6695 - val_loss: 2.4869 - val_mse: 115.8415\n",
            "Epoch 44/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4918 - mse: 120.1094 - val_loss: 2.4750 - val_mse: 118.8131\n",
            "Epoch 45/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4786 - mse: 118.7261 - val_loss: 2.5301 - val_mse: 118.5571\n",
            "Epoch 46/500\n",
            "560/560 [==============================] - 11s 20ms/step - loss: 2.4795 - mse: 118.7287 - val_loss: 2.4706 - val_mse: 115.7735\n",
            "Epoch 47/500\n",
            "560/560 [==============================] - 11s 20ms/step - loss: 2.4624 - mse: 117.4865 - val_loss: 2.5375 - val_mse: 132.0916\n",
            "Epoch 48/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4600 - mse: 117.4613 - val_loss: 2.4727 - val_mse: 120.3149\n",
            "Epoch 49/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.4445 - mse: 117.0027 - val_loss: 2.5379 - val_mse: 127.2289\n",
            "Epoch 50/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.4922 - mse: 119.5121 - val_loss: 2.4599 - val_mse: 119.9492\n",
            "Epoch 51/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.4593 - mse: 117.7341 - val_loss: 2.4492 - val_mse: 115.9619\n",
            "Epoch 52/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4505 - mse: 117.6872 - val_loss: 2.4576 - val_mse: 117.2991\n",
            "Epoch 53/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4680 - mse: 118.0962 - val_loss: 2.4540 - val_mse: 118.3442\n",
            "Epoch 54/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4554 - mse: 117.8398 - val_loss: 2.4468 - val_mse: 114.2382\n",
            "Epoch 55/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.4493 - mse: 116.8346 - val_loss: 2.4449 - val_mse: 118.9113\n",
            "Epoch 56/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4282 - mse: 115.3538 - val_loss: 2.4609 - val_mse: 112.2765\n",
            "Epoch 57/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4507 - mse: 116.7548 - val_loss: 2.4919 - val_mse: 110.3263\n",
            "Epoch 58/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4409 - mse: 116.3955 - val_loss: 2.4725 - val_mse: 112.2400\n",
            "Epoch 59/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4377 - mse: 116.3969 - val_loss: 2.4939 - val_mse: 109.6891\n",
            "Epoch 60/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4134 - mse: 115.2882 - val_loss: 2.4184 - val_mse: 117.3898\n",
            "Epoch 61/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3901 - mse: 114.2562 - val_loss: 2.4138 - val_mse: 115.4018\n",
            "Epoch 62/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3994 - mse: 115.2576 - val_loss: 2.4190 - val_mse: 119.4144\n",
            "Epoch 63/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4002 - mse: 115.0753 - val_loss: 2.4222 - val_mse: 120.1585\n",
            "Epoch 64/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4012 - mse: 115.1829 - val_loss: 2.4106 - val_mse: 114.0538\n",
            "Epoch 65/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4047 - mse: 115.8216 - val_loss: 2.4079 - val_mse: 115.1617\n",
            "Epoch 66/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3868 - mse: 113.8997 - val_loss: 2.4061 - val_mse: 116.0808\n",
            "Epoch 67/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3856 - mse: 114.1165 - val_loss: 2.4068 - val_mse: 113.2068\n",
            "Epoch 68/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3962 - mse: 114.7615 - val_loss: 2.4133 - val_mse: 112.4455\n",
            "Epoch 69/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4000 - mse: 115.0368 - val_loss: 2.4021 - val_mse: 113.6871\n",
            "Epoch 70/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3820 - mse: 113.9346 - val_loss: 2.4048 - val_mse: 112.8497\n",
            "Epoch 71/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4033 - mse: 115.2956 - val_loss: 2.3929 - val_mse: 115.5676\n",
            "Epoch 72/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3902 - mse: 114.8578 - val_loss: 2.4055 - val_mse: 113.8736\n",
            "Epoch 73/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3862 - mse: 114.1328 - val_loss: 2.4116 - val_mse: 118.1656\n",
            "Epoch 74/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3978 - mse: 115.1546 - val_loss: 2.3893 - val_mse: 114.5611\n",
            "Epoch 75/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3770 - mse: 113.6603 - val_loss: 2.4111 - val_mse: 111.2960\n",
            "Epoch 76/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3961 - mse: 114.7339 - val_loss: 2.3994 - val_mse: 112.8031\n",
            "Epoch 77/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3831 - mse: 113.7869 - val_loss: 2.3956 - val_mse: 113.7853\n",
            "Epoch 78/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3948 - mse: 114.6843 - val_loss: 2.3939 - val_mse: 112.7888\n",
            "Epoch 79/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3627 - mse: 112.9551 - val_loss: 2.3851 - val_mse: 111.4019\n",
            "Epoch 80/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3650 - mse: 113.3315 - val_loss: 2.3817 - val_mse: 116.8048\n",
            "Epoch 81/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3665 - mse: 113.6272 - val_loss: 2.3864 - val_mse: 110.7100\n",
            "Epoch 82/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3506 - mse: 111.8642 - val_loss: 2.3789 - val_mse: 112.8004\n",
            "Epoch 83/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3565 - mse: 112.5221 - val_loss: 2.3827 - val_mse: 116.6423\n",
            "Epoch 84/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3505 - mse: 112.1220 - val_loss: 2.3769 - val_mse: 112.5248\n",
            "Epoch 85/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3762 - mse: 113.7046 - val_loss: 2.3841 - val_mse: 111.2773\n",
            "Epoch 86/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3517 - mse: 112.3496 - val_loss: 2.3798 - val_mse: 111.9413\n",
            "Epoch 87/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3524 - mse: 112.2705 - val_loss: 2.3771 - val_mse: 112.2628\n",
            "Epoch 88/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3447 - mse: 111.7065 - val_loss: 2.3760 - val_mse: 112.0028\n",
            "Epoch 89/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3505 - mse: 112.7526 - val_loss: 2.3790 - val_mse: 112.0754\n",
            "Epoch 90/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3678 - mse: 114.0545 - val_loss: 2.3828 - val_mse: 117.8465\n",
            "Epoch 91/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3587 - mse: 112.9768 - val_loss: 2.3780 - val_mse: 114.6847\n",
            "Epoch 92/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3567 - mse: 112.8975 - val_loss: 2.3724 - val_mse: 112.8461\n",
            "Epoch 93/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3524 - mse: 112.3259 - val_loss: 2.3699 - val_mse: 112.1140\n",
            "Epoch 94/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3661 - mse: 113.3401 - val_loss: 2.3846 - val_mse: 111.7527\n",
            "Epoch 95/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3550 - mse: 112.5039 - val_loss: 2.3688 - val_mse: 113.2684\n",
            "Epoch 96/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3440 - mse: 112.0750 - val_loss: 2.3767 - val_mse: 110.6054\n",
            "Epoch 97/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3446 - mse: 111.6651 - val_loss: 2.3802 - val_mse: 116.5395\n",
            "Epoch 98/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3660 - mse: 113.2376 - val_loss: 2.3772 - val_mse: 111.0058\n",
            "Epoch 99/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3497 - mse: 112.1349 - val_loss: 2.3696 - val_mse: 111.5793\n",
            "Epoch 100/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.3601 - mse: 113.4462 - val_loss: 2.3630 - val_mse: 112.9097\n",
            "Epoch 101/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3499 - mse: 112.7931 - val_loss: 2.3618 - val_mse: 112.0477\n",
            "Epoch 102/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3327 - mse: 111.6629 - val_loss: 2.3656 - val_mse: 111.9427\n",
            "Epoch 103/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3203 - mse: 110.6997 - val_loss: 2.3608 - val_mse: 113.0080\n",
            "Epoch 104/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3389 - mse: 112.3082 - val_loss: 2.3615 - val_mse: 113.3838\n",
            "Epoch 105/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.3402 - mse: 112.0732 - val_loss: 2.3626 - val_mse: 111.6595\n",
            "Epoch 106/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3501 - mse: 112.5914 - val_loss: 2.3643 - val_mse: 111.9217\n",
            "Epoch 107/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3309 - mse: 111.3967 - val_loss: 2.3628 - val_mse: 114.1220\n",
            "Epoch 108/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3200 - mse: 110.7316 - val_loss: 2.3595 - val_mse: 111.8175\n",
            "Epoch 109/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3347 - mse: 111.2868 - val_loss: 2.3606 - val_mse: 111.4550\n",
            "Epoch 110/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3306 - mse: 111.3070 - val_loss: 2.3578 - val_mse: 112.2167\n",
            "Epoch 111/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3416 - mse: 112.2529 - val_loss: 2.3604 - val_mse: 110.9382\n",
            "Epoch 112/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3346 - mse: 111.7853 - val_loss: 2.3582 - val_mse: 113.3392\n",
            "Epoch 113/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3385 - mse: 112.2483 - val_loss: 2.3575 - val_mse: 112.9933\n",
            "Epoch 114/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3328 - mse: 111.9162 - val_loss: 2.3575 - val_mse: 112.3776\n",
            "Epoch 115/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.3381 - mse: 111.6850 - val_loss: 2.3588 - val_mse: 113.2678\n",
            "Epoch 116/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3377 - mse: 112.2737 - val_loss: 2.3575 - val_mse: 112.0678\n",
            "Epoch 117/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3454 - mse: 112.4820 - val_loss: 2.3570 - val_mse: 112.2472\n",
            "Epoch 118/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3215 - mse: 111.0054 - val_loss: 2.3563 - val_mse: 112.6948\n",
            "Epoch 119/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3496 - mse: 112.9495 - val_loss: 2.3572 - val_mse: 113.0763\n",
            "Epoch 120/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3262 - mse: 111.5146 - val_loss: 2.3559 - val_mse: 112.3719\n",
            "Epoch 121/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3279 - mse: 110.9739 - val_loss: 2.3563 - val_mse: 111.5897\n",
            "Epoch 122/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3288 - mse: 111.5450 - val_loss: 2.3580 - val_mse: 111.7140\n",
            "Epoch 123/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3465 - mse: 112.8595 - val_loss: 2.3565 - val_mse: 111.6537\n",
            "Epoch 124/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.3376 - mse: 112.4996 - val_loss: 2.3570 - val_mse: 111.6361\n",
            "Epoch 125/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3288 - mse: 111.4164 - val_loss: 2.3558 - val_mse: 111.7407\n",
            "Epoch 126/500\n",
            "560/560 [==============================] - 14s 25ms/step - loss: 2.3448 - mse: 112.8304 - val_loss: 2.3544 - val_mse: 112.7189\n",
            "Epoch 127/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 2.3350 - mse: 112.1655 - val_loss: 2.3569 - val_mse: 114.1146\n",
            "Epoch 128/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.3323 - mse: 111.6347 - val_loss: 2.3562 - val_mse: 111.4371\n",
            "Epoch 129/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.3328 - mse: 111.4934 - val_loss: 2.3547 - val_mse: 113.2762\n",
            "Epoch 130/500\n",
            "560/560 [==============================] - 14s 24ms/step - loss: 2.3361 - mse: 112.3470 - val_loss: 2.3551 - val_mse: 111.7247\n",
            "Epoch 131/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 2.3259 - mse: 111.0550 - val_loss: 2.3537 - val_mse: 112.4440\n",
            "Epoch 132/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3246 - mse: 111.6583 - val_loss: 2.3543 - val_mse: 112.0456\n",
            "Epoch 133/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3435 - mse: 112.6105 - val_loss: 2.3541 - val_mse: 112.2826\n",
            "Epoch 134/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3320 - mse: 111.2931 - val_loss: 2.3538 - val_mse: 112.3377\n",
            "Epoch 135/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3350 - mse: 112.0999 - val_loss: 2.3535 - val_mse: 112.6940\n",
            "Epoch 136/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3228 - mse: 111.1153 - val_loss: 2.3540 - val_mse: 112.2476\n",
            "Epoch 137/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3159 - mse: 110.2206 - val_loss: 2.3540 - val_mse: 111.9417\n",
            "Epoch 138/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3281 - mse: 111.7034 - val_loss: 2.3541 - val_mse: 112.4725\n",
            "Epoch 139/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3272 - mse: 111.8160 - val_loss: 2.3549 - val_mse: 111.3548\n",
            "Epoch 140/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3325 - mse: 111.6706 - val_loss: 2.3533 - val_mse: 112.2960\n",
            "Epoch 141/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3406 - mse: 112.2404 - val_loss: 2.3534 - val_mse: 112.4080\n",
            "Epoch 142/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3360 - mse: 112.3336 - val_loss: 2.3534 - val_mse: 112.2716\n",
            "Epoch 143/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3343 - mse: 111.7282 - val_loss: 2.3537 - val_mse: 111.9319\n",
            "Epoch 144/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3493 - mse: 112.7242 - val_loss: 2.3535 - val_mse: 111.9695\n",
            "Epoch 145/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3364 - mse: 111.8531 - val_loss: 2.3533 - val_mse: 112.2164\n",
            "Epoch 146/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3296 - mse: 112.0088 - val_loss: 2.3535 - val_mse: 112.0760\n",
            "Epoch 147/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3240 - mse: 110.9609 - val_loss: 2.3534 - val_mse: 112.1927\n",
            "Epoch 148/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3364 - mse: 111.9828 - val_loss: 2.3533 - val_mse: 112.1272\n",
            "Epoch 149/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3204 - mse: 110.8412 - val_loss: 2.3534 - val_mse: 112.0947\n",
            "Epoch 150/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3237 - mse: 110.9402 - val_loss: 2.3532 - val_mse: 112.2685\n",
            "Epoch 151/500\n",
            "560/560 [==============================] - 14s 24ms/step - loss: 2.3261 - mse: 110.9167 - val_loss: 2.3534 - val_mse: 112.0788\n",
            "Epoch 152/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3303 - mse: 111.4249 - val_loss: 2.3532 - val_mse: 112.2384\n",
            "Epoch 153/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3223 - mse: 110.9923 - val_loss: 2.3533 - val_mse: 112.0703\n",
            "Epoch 154/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3345 - mse: 112.0470 - val_loss: 2.3532 - val_mse: 112.2519\n",
            "Epoch 155/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3385 - mse: 112.2746 - val_loss: 2.3533 - val_mse: 112.0984\n",
            "Epoch 156/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3250 - mse: 111.5895 - val_loss: 2.3533 - val_mse: 112.0587\n",
            "Epoch 157/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 2.3202 - mse: 110.7675 - val_loss: 2.3533 - val_mse: 112.1088\n",
            "Epoch 158/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3340 - mse: 112.2461 - val_loss: 2.3532 - val_mse: 112.1575\n",
            "219/219 [==============================] - 1s 6ms/step - loss: 2.3379 - mse: 111.5983\n",
            "loss:  2.3378546237945557\n",
            "mae:  111.59833526611328\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f973b6f16a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "0.5번째 지정\n",
            "0.6번째 훈련\n",
            "Epoch 1/500\n",
            "560/560 [==============================] - 14s 22ms/step - loss: 9.7937 - mse: 1495.0693 - val_loss: 6.0525 - val_mse: 303.4069\n",
            "Epoch 2/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 5.9601 - mse: 291.6299 - val_loss: 5.8169 - val_mse: 284.3109\n",
            "Epoch 3/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 5.6693 - mse: 275.2912 - val_loss: 4.7150 - val_mse: 218.9549\n",
            "Epoch 4/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 4.1451 - mse: 201.2361 - val_loss: 3.3654 - val_mse: 158.0161\n",
            "Epoch 5/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 3.3030 - mse: 169.8752 - val_loss: 3.1659 - val_mse: 166.7892\n",
            "Epoch 6/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 3.1559 - mse: 166.3628 - val_loss: 3.0399 - val_mse: 163.0052\n",
            "Epoch 7/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 3.0346 - mse: 163.9069 - val_loss: 3.0055 - val_mse: 144.3937\n",
            "Epoch 8/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.9223 - mse: 161.3961 - val_loss: 2.8442 - val_mse: 158.5910\n",
            "Epoch 9/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.8485 - mse: 160.0195 - val_loss: 2.8355 - val_mse: 168.9797\n",
            "Epoch 10/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.8197 - mse: 162.1626 - val_loss: 2.7834 - val_mse: 165.6503\n",
            "Epoch 11/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.7965 - mse: 161.5512 - val_loss: 2.7928 - val_mse: 169.9718\n",
            "Epoch 12/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.7613 - mse: 161.5747 - val_loss: 2.7212 - val_mse: 150.1249\n",
            "Epoch 13/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.7144 - mse: 159.6959 - val_loss: 2.6826 - val_mse: 163.4321\n",
            "Epoch 14/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.6881 - mse: 158.9148 - val_loss: 2.7479 - val_mse: 181.6154\n",
            "Epoch 15/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.7100 - mse: 161.5993 - val_loss: 2.6691 - val_mse: 171.6173\n",
            "Epoch 16/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.6672 - mse: 160.0161 - val_loss: 2.6793 - val_mse: 174.6871\n",
            "Epoch 17/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.6618 - mse: 160.8161 - val_loss: 2.6458 - val_mse: 152.8215\n",
            "Epoch 18/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.6467 - mse: 161.1497 - val_loss: 2.6031 - val_mse: 150.7433\n",
            "Epoch 19/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.6041 - mse: 158.6751 - val_loss: 2.5970 - val_mse: 154.5856\n",
            "Epoch 20/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.5868 - mse: 157.5117 - val_loss: 2.5605 - val_mse: 158.6803\n",
            "Epoch 21/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.5730 - mse: 159.0144 - val_loss: 2.5593 - val_mse: 167.7665\n",
            "Epoch 22/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.5618 - mse: 158.8852 - val_loss: 2.6103 - val_mse: 138.3711\n",
            "Epoch 23/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.5410 - mse: 156.2374 - val_loss: 2.5627 - val_mse: 144.5527\n",
            "Epoch 24/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.5296 - mse: 155.9977 - val_loss: 2.5167 - val_mse: 165.1804\n",
            "Epoch 25/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.5486 - mse: 158.2490 - val_loss: 2.6098 - val_mse: 184.6909\n",
            "Epoch 26/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.5294 - mse: 157.3917 - val_loss: 2.5069 - val_mse: 152.8249\n",
            "Epoch 27/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.5255 - mse: 156.8172 - val_loss: 2.5662 - val_mse: 143.6556\n",
            "Epoch 28/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.5202 - mse: 157.0666 - val_loss: 2.5145 - val_mse: 156.3953\n",
            "Epoch 29/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.5150 - mse: 158.1906 - val_loss: 2.4561 - val_mse: 155.1514\n",
            "Epoch 30/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4951 - mse: 157.0138 - val_loss: 2.4881 - val_mse: 153.8424\n",
            "Epoch 31/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4900 - mse: 155.6834 - val_loss: 2.4699 - val_mse: 151.9640\n",
            "Epoch 32/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4606 - mse: 153.9022 - val_loss: 2.4503 - val_mse: 149.8091\n",
            "Epoch 33/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4665 - mse: 154.1888 - val_loss: 2.4631 - val_mse: 157.0080\n",
            "Epoch 34/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.4591 - mse: 153.6715 - val_loss: 2.4653 - val_mse: 145.3727\n",
            "Epoch 35/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4752 - mse: 154.9343 - val_loss: 2.4449 - val_mse: 157.9847\n",
            "Epoch 36/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4621 - mse: 154.5817 - val_loss: 2.4435 - val_mse: 144.3159\n",
            "Epoch 37/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.4477 - mse: 153.0708 - val_loss: 2.3955 - val_mse: 155.8300\n",
            "Epoch 38/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4256 - mse: 153.6854 - val_loss: 2.4201 - val_mse: 161.5807\n",
            "Epoch 39/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.4287 - mse: 152.5618 - val_loss: 2.4056 - val_mse: 160.8315\n",
            "Epoch 40/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.4039 - mse: 151.2155 - val_loss: 2.4243 - val_mse: 154.8716\n",
            "Epoch 41/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.4128 - mse: 151.2738 - val_loss: 2.4021 - val_mse: 156.3273\n",
            "Epoch 42/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.3695 - mse: 151.1274 - val_loss: 2.4041 - val_mse: 134.3795\n",
            "Epoch 43/500\n",
            "560/560 [==============================] - 14s 25ms/step - loss: 2.3485 - mse: 148.4551 - val_loss: 2.3424 - val_mse: 146.6243\n",
            "Epoch 44/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.3551 - mse: 148.5424 - val_loss: 2.4431 - val_mse: 174.7530\n",
            "Epoch 45/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3496 - mse: 148.3295 - val_loss: 2.3546 - val_mse: 138.7730\n",
            "Epoch 46/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3413 - mse: 148.2892 - val_loss: 2.3328 - val_mse: 149.3020\n",
            "Epoch 47/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3463 - mse: 148.0633 - val_loss: 2.3396 - val_mse: 138.6057\n",
            "Epoch 48/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3373 - mse: 146.8255 - val_loss: 2.3317 - val_mse: 151.5714\n",
            "Epoch 49/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3386 - mse: 147.4933 - val_loss: 2.3358 - val_mse: 138.0920\n",
            "Epoch 50/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3387 - mse: 146.7449 - val_loss: 2.3341 - val_mse: 141.5931\n",
            "Epoch 51/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3374 - mse: 146.4602 - val_loss: 2.3339 - val_mse: 143.6612\n",
            "Epoch 52/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3221 - mse: 145.6757 - val_loss: 2.3397 - val_mse: 153.6006\n",
            "Epoch 53/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3123 - mse: 146.3874 - val_loss: 2.3046 - val_mse: 144.2441\n",
            "Epoch 54/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3135 - mse: 146.6502 - val_loss: 2.3183 - val_mse: 138.9318\n",
            "Epoch 55/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3060 - mse: 145.5937 - val_loss: 2.3089 - val_mse: 139.1341\n",
            "Epoch 56/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2903 - mse: 143.4170 - val_loss: 2.2926 - val_mse: 142.5246\n",
            "Epoch 57/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3057 - mse: 145.6611 - val_loss: 2.2937 - val_mse: 144.1183\n",
            "Epoch 58/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3054 - mse: 145.6830 - val_loss: 2.3008 - val_mse: 137.0623\n",
            "Epoch 59/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3009 - mse: 145.2934 - val_loss: 2.2950 - val_mse: 144.5485\n",
            "Epoch 60/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2994 - mse: 145.0939 - val_loss: 2.2904 - val_mse: 141.3691\n",
            "Epoch 61/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.3085 - mse: 146.1247 - val_loss: 2.2982 - val_mse: 153.7150\n",
            "Epoch 62/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2888 - mse: 144.5917 - val_loss: 2.2871 - val_mse: 144.9750\n",
            "Epoch 63/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2860 - mse: 145.1938 - val_loss: 2.2825 - val_mse: 142.5769\n",
            "Epoch 64/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2910 - mse: 145.0530 - val_loss: 2.2820 - val_mse: 142.3359\n",
            "Epoch 65/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2794 - mse: 143.4868 - val_loss: 2.2987 - val_mse: 150.6790\n",
            "Epoch 66/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2861 - mse: 143.9248 - val_loss: 2.2778 - val_mse: 141.9172\n",
            "Epoch 67/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2980 - mse: 145.5460 - val_loss: 2.2822 - val_mse: 142.7132\n",
            "Epoch 68/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2833 - mse: 144.2654 - val_loss: 2.2834 - val_mse: 148.3434\n",
            "Epoch 69/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2764 - mse: 144.2778 - val_loss: 2.2765 - val_mse: 145.9759\n",
            "Epoch 70/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2930 - mse: 145.0723 - val_loss: 2.2835 - val_mse: 143.1477\n",
            "Epoch 71/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2790 - mse: 144.4942 - val_loss: 2.2744 - val_mse: 146.6966\n",
            "Epoch 72/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2719 - mse: 143.6453 - val_loss: 2.2831 - val_mse: 136.8742\n",
            "Epoch 73/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2866 - mse: 143.9773 - val_loss: 2.2651 - val_mse: 143.7567\n",
            "Epoch 74/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2793 - mse: 144.5821 - val_loss: 2.2780 - val_mse: 138.1385\n",
            "Epoch 75/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2807 - mse: 143.9911 - val_loss: 2.2647 - val_mse: 145.8350\n",
            "Epoch 76/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2670 - mse: 143.1564 - val_loss: 2.2643 - val_mse: 140.5941\n",
            "Epoch 77/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2764 - mse: 143.7210 - val_loss: 2.2635 - val_mse: 143.9023\n",
            "Epoch 78/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2704 - mse: 143.3936 - val_loss: 2.2695 - val_mse: 137.8627\n",
            "Epoch 79/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.2842 - mse: 144.3146 - val_loss: 2.2639 - val_mse: 146.7783\n",
            "Epoch 80/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2625 - mse: 143.1732 - val_loss: 2.2646 - val_mse: 138.2388\n",
            "Epoch 81/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2677 - mse: 143.4012 - val_loss: 2.2634 - val_mse: 145.0002\n",
            "Epoch 82/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2631 - mse: 141.8443 - val_loss: 2.2574 - val_mse: 140.8188\n",
            "Epoch 83/500\n",
            "560/560 [==============================] - 11s 20ms/step - loss: 2.2621 - mse: 142.3760 - val_loss: 2.2662 - val_mse: 151.9310\n",
            "Epoch 84/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2779 - mse: 143.9270 - val_loss: 2.2586 - val_mse: 140.4640\n",
            "Epoch 85/500\n",
            "560/560 [==============================] - 11s 21ms/step - loss: 2.2616 - mse: 142.3931 - val_loss: 2.2553 - val_mse: 145.4371\n",
            "Epoch 86/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2645 - mse: 142.9466 - val_loss: 2.2592 - val_mse: 146.9237\n",
            "Epoch 87/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2736 - mse: 144.0391 - val_loss: 2.2551 - val_mse: 139.4849\n",
            "Epoch 88/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2665 - mse: 143.2013 - val_loss: 2.2495 - val_mse: 142.9249\n",
            "Epoch 89/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2688 - mse: 143.3014 - val_loss: 2.2744 - val_mse: 129.9864\n",
            "Epoch 90/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.2480 - mse: 140.7655 - val_loss: 2.2535 - val_mse: 137.4933\n",
            "Epoch 91/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2693 - mse: 142.0498 - val_loss: 2.2517 - val_mse: 144.0767\n",
            "Epoch 92/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2417 - mse: 140.9064 - val_loss: 2.2579 - val_mse: 135.7985\n",
            "Epoch 93/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.2458 - mse: 141.3011 - val_loss: 2.2373 - val_mse: 138.3824\n",
            "Epoch 94/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.2451 - mse: 142.3193 - val_loss: 2.2395 - val_mse: 142.4093\n",
            "Epoch 95/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2515 - mse: 142.7551 - val_loss: 2.2427 - val_mse: 146.2845\n",
            "Epoch 96/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2521 - mse: 142.3845 - val_loss: 2.2353 - val_mse: 142.6596\n",
            "Epoch 97/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.2434 - mse: 141.9574 - val_loss: 2.2361 - val_mse: 141.9449\n",
            "Epoch 98/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.2407 - mse: 141.6094 - val_loss: 2.2338 - val_mse: 142.0190\n",
            "Epoch 99/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2334 - mse: 140.9509 - val_loss: 2.2344 - val_mse: 143.7314\n",
            "Epoch 100/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2336 - mse: 141.2451 - val_loss: 2.2389 - val_mse: 134.5651\n",
            "Epoch 101/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2331 - mse: 140.5546 - val_loss: 2.2303 - val_mse: 139.0941\n",
            "Epoch 102/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2350 - mse: 140.5761 - val_loss: 2.2388 - val_mse: 135.7437\n",
            "Epoch 103/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2345 - mse: 140.2021 - val_loss: 2.2295 - val_mse: 139.7420\n",
            "Epoch 104/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2239 - mse: 139.8222 - val_loss: 2.2331 - val_mse: 141.6917\n",
            "Epoch 105/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.2327 - mse: 141.4832 - val_loss: 2.2316 - val_mse: 142.9927\n",
            "Epoch 106/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.2268 - mse: 140.2274 - val_loss: 2.2286 - val_mse: 138.8078\n",
            "Epoch 107/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2125 - mse: 138.9858 - val_loss: 2.2281 - val_mse: 143.3905\n",
            "Epoch 108/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2181 - mse: 139.7177 - val_loss: 2.2278 - val_mse: 140.0618\n",
            "Epoch 109/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2184 - mse: 139.6715 - val_loss: 2.2306 - val_mse: 140.6278\n",
            "Epoch 110/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2416 - mse: 141.4634 - val_loss: 2.2435 - val_mse: 132.1414\n",
            "Epoch 111/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2311 - mse: 140.5658 - val_loss: 2.2281 - val_mse: 142.1643\n",
            "Epoch 112/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2320 - mse: 141.2756 - val_loss: 2.2279 - val_mse: 142.2867\n",
            "Epoch 113/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2127 - mse: 139.2503 - val_loss: 2.2215 - val_mse: 137.2449\n",
            "Epoch 114/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.2139 - mse: 138.9359 - val_loss: 2.2211 - val_mse: 138.4409\n",
            "Epoch 115/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.2100 - mse: 139.0516 - val_loss: 2.2207 - val_mse: 140.7574\n",
            "Epoch 116/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2248 - mse: 140.5060 - val_loss: 2.2210 - val_mse: 141.7797\n",
            "Epoch 117/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.2268 - mse: 140.9164 - val_loss: 2.2199 - val_mse: 137.5597\n",
            "Epoch 118/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2254 - mse: 140.5103 - val_loss: 2.2217 - val_mse: 135.4530\n",
            "Epoch 119/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 2.2240 - mse: 140.6026 - val_loss: 2.2194 - val_mse: 136.9196\n",
            "Epoch 120/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2212 - mse: 139.7227 - val_loss: 2.2193 - val_mse: 140.3870\n",
            "Epoch 121/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2226 - mse: 141.1283 - val_loss: 2.2254 - val_mse: 135.3066\n",
            "Epoch 122/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.2330 - mse: 141.2702 - val_loss: 2.2191 - val_mse: 139.6903\n",
            "Epoch 123/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.2132 - mse: 139.5001 - val_loss: 2.2190 - val_mse: 138.8255\n",
            "Epoch 124/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2252 - mse: 140.8136 - val_loss: 2.2215 - val_mse: 134.5755\n",
            "Epoch 125/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2143 - mse: 138.9457 - val_loss: 2.2172 - val_mse: 139.6011\n",
            "Epoch 126/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2180 - mse: 140.6568 - val_loss: 2.2189 - val_mse: 136.1225\n",
            "Epoch 127/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2162 - mse: 140.0470 - val_loss: 2.2167 - val_mse: 136.9041\n",
            "Epoch 128/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2135 - mse: 140.1543 - val_loss: 2.2165 - val_mse: 139.7728\n",
            "Epoch 129/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2091 - mse: 138.6045 - val_loss: 2.2152 - val_mse: 139.2234\n",
            "Epoch 130/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2123 - mse: 139.9459 - val_loss: 2.2150 - val_mse: 137.8970\n",
            "Epoch 131/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.2035 - mse: 138.4663 - val_loss: 2.2151 - val_mse: 139.8298\n",
            "Epoch 132/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.2058 - mse: 138.6963 - val_loss: 2.2161 - val_mse: 138.1907\n",
            "Epoch 133/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2237 - mse: 140.8918 - val_loss: 2.2139 - val_mse: 138.3658\n",
            "Epoch 134/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2172 - mse: 139.7342 - val_loss: 2.2148 - val_mse: 140.7677\n",
            "Epoch 135/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2078 - mse: 139.2071 - val_loss: 2.2185 - val_mse: 134.3376\n",
            "Epoch 136/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2142 - mse: 139.1248 - val_loss: 2.2138 - val_mse: 141.5043\n",
            "Epoch 137/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2074 - mse: 138.5895 - val_loss: 2.2175 - val_mse: 143.0004\n",
            "Epoch 138/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2163 - mse: 139.3041 - val_loss: 2.2122 - val_mse: 136.9184\n",
            "Epoch 139/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.2020 - mse: 138.8068 - val_loss: 2.2124 - val_mse: 135.8218\n",
            "Epoch 140/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2201 - mse: 140.3811 - val_loss: 2.2115 - val_mse: 137.1849\n",
            "Epoch 141/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.2188 - mse: 140.4918 - val_loss: 2.2101 - val_mse: 137.4238\n",
            "Epoch 142/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.1996 - mse: 138.4748 - val_loss: 2.2159 - val_mse: 133.7626\n",
            "Epoch 143/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2117 - mse: 139.6216 - val_loss: 2.2092 - val_mse: 138.3694\n",
            "Epoch 144/500\n",
            "560/560 [==============================] - 14s 24ms/step - loss: 2.2249 - mse: 140.7175 - val_loss: 2.2105 - val_mse: 136.8691\n",
            "Epoch 145/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2160 - mse: 139.0055 - val_loss: 2.2123 - val_mse: 142.3142\n",
            "Epoch 146/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.2051 - mse: 139.5719 - val_loss: 2.2105 - val_mse: 138.4718\n",
            "Epoch 147/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2161 - mse: 139.9208 - val_loss: 2.2107 - val_mse: 141.0298\n",
            "Epoch 148/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2134 - mse: 140.1044 - val_loss: 2.2086 - val_mse: 138.7730\n",
            "Epoch 149/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.2126 - mse: 140.1217 - val_loss: 2.2096 - val_mse: 136.2258\n",
            "Epoch 150/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.2062 - mse: 139.1970 - val_loss: 2.2084 - val_mse: 139.2698\n",
            "Epoch 151/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.1976 - mse: 138.6512 - val_loss: 2.2078 - val_mse: 138.0641\n",
            "Epoch 152/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.1953 - mse: 138.3338 - val_loss: 2.2083 - val_mse: 137.9515\n",
            "Epoch 153/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2016 - mse: 138.2019 - val_loss: 2.2082 - val_mse: 139.9124\n",
            "Epoch 154/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2034 - mse: 139.6221 - val_loss: 2.2090 - val_mse: 139.2463\n",
            "Epoch 155/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2235 - mse: 141.0120 - val_loss: 2.2082 - val_mse: 137.6950\n",
            "Epoch 156/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2060 - mse: 139.5210 - val_loss: 2.2077 - val_mse: 139.6309\n",
            "Epoch 157/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.1885 - mse: 137.3737 - val_loss: 2.2085 - val_mse: 136.3984\n",
            "Epoch 158/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 2.2076 - mse: 139.2633 - val_loss: 2.2083 - val_mse: 140.2276\n",
            "Epoch 159/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1976 - mse: 138.3315 - val_loss: 2.2085 - val_mse: 136.2940\n",
            "Epoch 160/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2037 - mse: 139.0074 - val_loss: 2.2079 - val_mse: 139.5652\n",
            "Epoch 161/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1953 - mse: 137.9691 - val_loss: 2.2072 - val_mse: 138.1657\n",
            "Epoch 162/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2176 - mse: 139.6129 - val_loss: 2.2072 - val_mse: 137.9731\n",
            "Epoch 163/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2136 - mse: 139.7768 - val_loss: 2.2071 - val_mse: 137.9948\n",
            "Epoch 164/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2097 - mse: 139.7718 - val_loss: 2.2071 - val_mse: 138.3401\n",
            "Epoch 165/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2037 - mse: 138.8612 - val_loss: 2.2072 - val_mse: 139.1706\n",
            "Epoch 166/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2051 - mse: 139.5468 - val_loss: 2.2070 - val_mse: 138.6226\n",
            "Epoch 167/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1914 - mse: 138.4959 - val_loss: 2.2070 - val_mse: 138.5435\n",
            "Epoch 168/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2049 - mse: 138.6694 - val_loss: 2.2072 - val_mse: 138.8514\n",
            "Epoch 169/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.2066 - mse: 139.4266 - val_loss: 2.2073 - val_mse: 137.0247\n",
            "Epoch 170/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2182 - mse: 140.2489 - val_loss: 2.2069 - val_mse: 138.2220\n",
            "Epoch 171/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2055 - mse: 139.3444 - val_loss: 2.2069 - val_mse: 137.7740\n",
            "Epoch 172/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2152 - mse: 139.3906 - val_loss: 2.2070 - val_mse: 137.3083\n",
            "Epoch 173/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1954 - mse: 137.6063 - val_loss: 2.2069 - val_mse: 137.9352\n",
            "Epoch 174/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.1921 - mse: 137.4070 - val_loss: 2.2068 - val_mse: 137.9975\n",
            "Epoch 175/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2129 - mse: 139.5032 - val_loss: 2.2069 - val_mse: 138.0359\n",
            "Epoch 176/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2110 - mse: 139.8640 - val_loss: 2.2070 - val_mse: 137.3781\n",
            "Epoch 177/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.1865 - mse: 137.2866 - val_loss: 2.2068 - val_mse: 137.9302\n",
            "Epoch 178/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2142 - mse: 140.0217 - val_loss: 2.2066 - val_mse: 138.5842\n",
            "Epoch 179/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.2040 - mse: 139.0643 - val_loss: 2.2066 - val_mse: 138.0442\n",
            "Epoch 180/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2017 - mse: 139.6535 - val_loss: 2.2067 - val_mse: 138.2201\n",
            "Epoch 181/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1995 - mse: 139.0832 - val_loss: 2.2069 - val_mse: 137.3973\n",
            "Epoch 182/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2008 - mse: 138.8152 - val_loss: 2.2066 - val_mse: 138.3001\n",
            "Epoch 183/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.1984 - mse: 138.9214 - val_loss: 2.2066 - val_mse: 138.0181\n",
            "Epoch 184/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 2.2027 - mse: 138.6737 - val_loss: 2.2066 - val_mse: 138.2687\n",
            "Epoch 185/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2043 - mse: 139.2587 - val_loss: 2.2067 - val_mse: 137.7887\n",
            "Epoch 186/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2059 - mse: 139.3916 - val_loss: 2.2066 - val_mse: 138.3360\n",
            "219/219 [==============================] - 1s 7ms/step - loss: 2.1944 - mse: 138.0967\n",
            "loss:  2.1943864822387695\n",
            "mae:  138.09666442871094\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9738c49488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "0.6번째 지정\n",
            "0.7번째 훈련\n",
            "Epoch 1/500\n",
            "560/560 [==============================] - 14s 23ms/step - loss: 8.0969 - mse: 600.8440 - val_loss: 2.7019 - val_mse: 182.5779\n",
            "Epoch 2/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.6747 - mse: 198.0509 - val_loss: 2.6605 - val_mse: 170.7201\n",
            "Epoch 3/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.5716 - mse: 194.9900 - val_loss: 2.5464 - val_mse: 193.6513\n",
            "Epoch 4/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.4729 - mse: 195.6732 - val_loss: 2.4988 - val_mse: 205.7557\n",
            "Epoch 5/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.3651 - mse: 194.3799 - val_loss: 2.4088 - val_mse: 239.4394\n",
            "Epoch 6/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.2459 - mse: 191.0631 - val_loss: 2.1715 - val_mse: 195.0050\n",
            "Epoch 7/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.1956 - mse: 191.0458 - val_loss: 2.1915 - val_mse: 179.7128\n",
            "Epoch 8/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.1649 - mse: 188.9615 - val_loss: 2.1416 - val_mse: 191.2967\n",
            "Epoch 9/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1448 - mse: 192.2107 - val_loss: 2.1473 - val_mse: 199.7968\n",
            "Epoch 10/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.1088 - mse: 188.9259 - val_loss: 2.1349 - val_mse: 172.3731\n",
            "Epoch 11/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1055 - mse: 188.8015 - val_loss: 2.1125 - val_mse: 200.7300\n",
            "Epoch 12/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.1234 - mse: 192.3709 - val_loss: 2.1246 - val_mse: 165.3877\n",
            "Epoch 13/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.0891 - mse: 188.1067 - val_loss: 2.0817 - val_mse: 177.9971\n",
            "Epoch 14/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.0927 - mse: 189.7848 - val_loss: 2.0558 - val_mse: 192.1342\n",
            "Epoch 15/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.0814 - mse: 189.2765 - val_loss: 2.0968 - val_mse: 191.8892\n",
            "Epoch 16/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.0720 - mse: 186.9803 - val_loss: 2.0710 - val_mse: 185.2442\n",
            "Epoch 17/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.0690 - mse: 188.5055 - val_loss: 2.1185 - val_mse: 200.4456\n",
            "Epoch 18/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.0723 - mse: 188.5498 - val_loss: 2.0387 - val_mse: 187.8909\n",
            "Epoch 19/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.0581 - mse: 187.0587 - val_loss: 2.0952 - val_mse: 165.3560\n",
            "Epoch 20/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.0281 - mse: 182.7633 - val_loss: 2.0335 - val_mse: 183.6996\n",
            "Epoch 21/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.0463 - mse: 185.4073 - val_loss: 2.0883 - val_mse: 207.7232\n",
            "Epoch 22/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.0375 - mse: 184.7933 - val_loss: 2.0080 - val_mse: 179.9258\n",
            "Epoch 23/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.0249 - mse: 182.0685 - val_loss: 2.0247 - val_mse: 175.1435\n",
            "Epoch 24/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.0311 - mse: 184.2684 - val_loss: 2.1153 - val_mse: 156.8558\n",
            "Epoch 25/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 2.0244 - mse: 181.4898 - val_loss: 2.0629 - val_mse: 191.2678\n",
            "Epoch 26/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.0189 - mse: 182.7338 - val_loss: 2.0319 - val_mse: 180.0449\n",
            "Epoch 27/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.9870 - mse: 183.1273 - val_loss: 1.9712 - val_mse: 184.2222\n",
            "Epoch 28/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.9681 - mse: 181.4584 - val_loss: 2.0137 - val_mse: 199.1754\n",
            "Epoch 29/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.9763 - mse: 181.5582 - val_loss: 1.9608 - val_mse: 180.0103\n",
            "Epoch 30/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.9525 - mse: 179.6654 - val_loss: 2.0224 - val_mse: 210.8003\n",
            "Epoch 31/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.9687 - mse: 182.4767 - val_loss: 1.9643 - val_mse: 182.7869\n",
            "Epoch 32/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.9431 - mse: 178.8919 - val_loss: 1.9493 - val_mse: 178.2473\n",
            "Epoch 33/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.9460 - mse: 179.3918 - val_loss: 1.9746 - val_mse: 200.9029\n",
            "Epoch 34/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.9447 - mse: 180.6322 - val_loss: 1.9372 - val_mse: 170.0007\n",
            "Epoch 35/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.9538 - mse: 181.6653 - val_loss: 1.9923 - val_mse: 195.9912\n",
            "Epoch 36/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.9410 - mse: 179.9162 - val_loss: 1.9368 - val_mse: 176.9186\n",
            "Epoch 37/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.9315 - mse: 178.0145 - val_loss: 1.9375 - val_mse: 179.4796\n",
            "Epoch 38/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.9284 - mse: 178.7084 - val_loss: 1.9218 - val_mse: 173.3435\n",
            "Epoch 39/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.9170 - mse: 177.2707 - val_loss: 1.9295 - val_mse: 187.1116\n",
            "Epoch 40/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.9338 - mse: 178.6014 - val_loss: 1.9460 - val_mse: 167.2080\n",
            "Epoch 41/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.9177 - mse: 176.8372 - val_loss: 1.9263 - val_mse: 184.2534\n",
            "Epoch 42/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.9327 - mse: 179.4759 - val_loss: 1.9090 - val_mse: 168.0269\n",
            "Epoch 43/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.9145 - mse: 176.2284 - val_loss: 1.9332 - val_mse: 189.8903\n",
            "Epoch 44/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.9297 - mse: 179.6328 - val_loss: 1.9482 - val_mse: 158.9005\n",
            "Epoch 45/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.9140 - mse: 175.6694 - val_loss: 1.9335 - val_mse: 185.7357\n",
            "Epoch 46/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.9204 - mse: 177.7205 - val_loss: 1.9448 - val_mse: 160.4414\n",
            "Epoch 47/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8878 - mse: 174.8577 - val_loss: 1.8887 - val_mse: 175.8577\n",
            "Epoch 48/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8808 - mse: 174.6739 - val_loss: 1.8823 - val_mse: 174.1444\n",
            "Epoch 49/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.8904 - mse: 175.9808 - val_loss: 1.8872 - val_mse: 170.3929\n",
            "Epoch 50/500\n",
            "560/560 [==============================] - 14s 26ms/step - loss: 1.8895 - mse: 175.2504 - val_loss: 1.8962 - val_mse: 178.0996\n",
            "Epoch 51/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.8724 - mse: 173.3093 - val_loss: 1.8813 - val_mse: 175.5154\n",
            "Epoch 52/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8877 - mse: 175.2864 - val_loss: 1.8814 - val_mse: 167.3100\n",
            "Epoch 53/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8751 - mse: 173.4350 - val_loss: 1.8794 - val_mse: 167.3778\n",
            "Epoch 54/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8832 - mse: 173.4624 - val_loss: 1.8922 - val_mse: 179.4278\n",
            "Epoch 55/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.8822 - mse: 173.8190 - val_loss: 1.8775 - val_mse: 165.7990\n",
            "Epoch 56/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8709 - mse: 172.5793 - val_loss: 1.8811 - val_mse: 164.0502\n",
            "Epoch 57/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.8772 - mse: 173.3827 - val_loss: 1.8833 - val_mse: 170.5421\n",
            "Epoch 58/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.8742 - mse: 173.4118 - val_loss: 1.8842 - val_mse: 183.4807\n",
            "Epoch 59/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8668 - mse: 172.3002 - val_loss: 1.8822 - val_mse: 177.3145\n",
            "Epoch 60/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8579 - mse: 171.9615 - val_loss: 1.8564 - val_mse: 169.7937\n",
            "Epoch 61/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8513 - mse: 172.3865 - val_loss: 1.8561 - val_mse: 174.0071\n",
            "Epoch 62/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8591 - mse: 172.5161 - val_loss: 1.8536 - val_mse: 170.5554\n",
            "Epoch 63/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8535 - mse: 171.8500 - val_loss: 1.8647 - val_mse: 164.1975\n",
            "Epoch 64/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8413 - mse: 170.2307 - val_loss: 1.8657 - val_mse: 162.9016\n",
            "Epoch 65/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8685 - mse: 172.7498 - val_loss: 1.8511 - val_mse: 169.8729\n",
            "Epoch 66/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.8447 - mse: 170.5901 - val_loss: 1.8520 - val_mse: 169.8137\n",
            "Epoch 67/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8535 - mse: 172.0859 - val_loss: 1.8528 - val_mse: 169.6530\n",
            "Epoch 68/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.8451 - mse: 170.5609 - val_loss: 1.8510 - val_mse: 168.2930\n",
            "Epoch 69/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8451 - mse: 170.3022 - val_loss: 1.8474 - val_mse: 171.0275\n",
            "Epoch 70/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8351 - mse: 169.2184 - val_loss: 1.8574 - val_mse: 165.7343\n",
            "Epoch 71/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.8502 - mse: 171.5810 - val_loss: 1.8513 - val_mse: 172.9128\n",
            "Epoch 72/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8367 - mse: 169.4323 - val_loss: 1.8494 - val_mse: 165.2009\n",
            "Epoch 73/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8485 - mse: 171.2006 - val_loss: 1.8565 - val_mse: 176.3224\n",
            "Epoch 74/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.8461 - mse: 171.5597 - val_loss: 1.8410 - val_mse: 168.4772\n",
            "Epoch 75/500\n",
            "560/560 [==============================] - 14s 24ms/step - loss: 1.8202 - mse: 168.6879 - val_loss: 1.8415 - val_mse: 170.7735\n",
            "Epoch 76/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8392 - mse: 171.1022 - val_loss: 1.8439 - val_mse: 164.3187\n",
            "Epoch 77/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8432 - mse: 170.6577 - val_loss: 1.8411 - val_mse: 168.4547\n",
            "Epoch 78/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.8475 - mse: 172.4921 - val_loss: 1.8405 - val_mse: 168.4066\n",
            "Epoch 79/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8576 - mse: 173.5009 - val_loss: 1.8398 - val_mse: 171.8042\n",
            "Epoch 80/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8273 - mse: 169.1814 - val_loss: 1.8403 - val_mse: 168.8576\n",
            "Epoch 81/500\n",
            "560/560 [==============================] - 14s 25ms/step - loss: 1.8362 - mse: 169.9378 - val_loss: 1.8390 - val_mse: 167.5485\n",
            "Epoch 82/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8398 - mse: 170.4955 - val_loss: 1.8379 - val_mse: 168.6426\n",
            "Epoch 83/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8361 - mse: 169.9267 - val_loss: 1.8377 - val_mse: 167.7292\n",
            "Epoch 84/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8373 - mse: 170.1770 - val_loss: 1.8385 - val_mse: 168.5695\n",
            "Epoch 85/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8387 - mse: 171.2730 - val_loss: 1.8367 - val_mse: 167.8017\n",
            "Epoch 86/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8259 - mse: 169.0252 - val_loss: 1.8384 - val_mse: 166.3593\n",
            "Epoch 87/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.8456 - mse: 171.9155 - val_loss: 1.8364 - val_mse: 167.8044\n",
            "Epoch 88/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8378 - mse: 170.0167 - val_loss: 1.8391 - val_mse: 164.7114\n",
            "Epoch 89/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8214 - mse: 168.3447 - val_loss: 1.8354 - val_mse: 165.4166\n",
            "Epoch 90/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.8393 - mse: 170.4945 - val_loss: 1.8371 - val_mse: 165.2882\n",
            "Epoch 91/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8243 - mse: 168.1800 - val_loss: 1.8365 - val_mse: 166.3241\n",
            "Epoch 92/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.8415 - mse: 170.9022 - val_loss: 1.8339 - val_mse: 166.0945\n",
            "Epoch 93/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8252 - mse: 168.8082 - val_loss: 1.8348 - val_mse: 166.5716\n",
            "Epoch 94/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8176 - mse: 167.3843 - val_loss: 1.8360 - val_mse: 169.2372\n",
            "Epoch 95/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8286 - mse: 168.9638 - val_loss: 1.8324 - val_mse: 168.8950\n",
            "Epoch 96/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8351 - mse: 170.0974 - val_loss: 1.8311 - val_mse: 167.3166\n",
            "Epoch 97/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.8267 - mse: 168.7653 - val_loss: 1.8363 - val_mse: 169.2453\n",
            "Epoch 98/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.8211 - mse: 167.4764 - val_loss: 1.8338 - val_mse: 165.5290\n",
            "Epoch 99/500\n",
            "560/560 [==============================] - 14s 24ms/step - loss: 1.8303 - mse: 169.3398 - val_loss: 1.8303 - val_mse: 167.7415\n",
            "Epoch 100/500\n",
            "560/560 [==============================] - 15s 27ms/step - loss: 1.8291 - mse: 168.7302 - val_loss: 1.8317 - val_mse: 165.7851\n",
            "Epoch 101/500\n",
            "560/560 [==============================] - 15s 27ms/step - loss: 1.8115 - mse: 166.5534 - val_loss: 1.8337 - val_mse: 169.8769\n",
            "Epoch 102/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.8260 - mse: 169.6169 - val_loss: 1.8322 - val_mse: 171.6163\n",
            "Epoch 103/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.8220 - mse: 168.6468 - val_loss: 1.8327 - val_mse: 166.4575\n",
            "Epoch 104/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.8302 - mse: 170.3269 - val_loss: 1.8278 - val_mse: 167.6187\n",
            "Epoch 105/500\n",
            "560/560 [==============================] - 14s 25ms/step - loss: 1.8270 - mse: 169.4862 - val_loss: 1.8269 - val_mse: 168.7370\n",
            "Epoch 106/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.8348 - mse: 171.0661 - val_loss: 1.8273 - val_mse: 167.5396\n",
            "Epoch 107/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8215 - mse: 168.6169 - val_loss: 1.8271 - val_mse: 170.8992\n",
            "Epoch 108/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8115 - mse: 167.5499 - val_loss: 1.8262 - val_mse: 167.9244\n",
            "Epoch 109/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8296 - mse: 170.1363 - val_loss: 1.8274 - val_mse: 170.7498\n",
            "Epoch 110/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8264 - mse: 169.1986 - val_loss: 1.8269 - val_mse: 167.6698\n",
            "Epoch 111/500\n",
            "560/560 [==============================] - 14s 24ms/step - loss: 1.8230 - mse: 169.1032 - val_loss: 1.8263 - val_mse: 170.2132\n",
            "Epoch 112/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8416 - mse: 171.4885 - val_loss: 1.8257 - val_mse: 168.1409\n",
            "Epoch 113/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8215 - mse: 169.3693 - val_loss: 1.8269 - val_mse: 164.4808\n",
            "Epoch 114/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8167 - mse: 167.8319 - val_loss: 1.8261 - val_mse: 164.6787\n",
            "Epoch 115/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8234 - mse: 168.7113 - val_loss: 1.8278 - val_mse: 163.5663\n",
            "Epoch 116/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8146 - mse: 166.7127 - val_loss: 1.8271 - val_mse: 165.6993\n",
            "Epoch 117/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8343 - mse: 169.6582 - val_loss: 1.8245 - val_mse: 169.1500\n",
            "Epoch 118/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8129 - mse: 167.8551 - val_loss: 1.8244 - val_mse: 167.0608\n",
            "Epoch 119/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8179 - mse: 168.6715 - val_loss: 1.8256 - val_mse: 164.6711\n",
            "Epoch 120/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8136 - mse: 167.4096 - val_loss: 1.8242 - val_mse: 168.9633\n",
            "Epoch 121/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8101 - mse: 167.1606 - val_loss: 1.8242 - val_mse: 165.9224\n",
            "Epoch 122/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8262 - mse: 169.8523 - val_loss: 1.8240 - val_mse: 168.0150\n",
            "Epoch 123/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8189 - mse: 167.9114 - val_loss: 1.8241 - val_mse: 166.9898\n",
            "Epoch 124/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.8121 - mse: 167.6317 - val_loss: 1.8243 - val_mse: 166.7263\n",
            "Epoch 125/500\n",
            "560/560 [==============================] - 14s 25ms/step - loss: 1.8194 - mse: 168.4150 - val_loss: 1.8235 - val_mse: 167.1448\n",
            "Epoch 126/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8083 - mse: 167.2411 - val_loss: 1.8245 - val_mse: 164.8545\n",
            "Epoch 127/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8205 - mse: 169.1725 - val_loss: 1.8232 - val_mse: 168.0903\n",
            "Epoch 128/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8159 - mse: 167.6826 - val_loss: 1.8241 - val_mse: 165.0226\n",
            "Epoch 129/500\n",
            "560/560 [==============================] - 14s 25ms/step - loss: 1.8260 - mse: 169.6595 - val_loss: 1.8231 - val_mse: 169.2141\n",
            "Epoch 130/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.8127 - mse: 168.1328 - val_loss: 1.8226 - val_mse: 167.0806\n",
            "Epoch 131/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8328 - mse: 170.9909 - val_loss: 1.8224 - val_mse: 167.1232\n",
            "Epoch 132/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8211 - mse: 168.8269 - val_loss: 1.8229 - val_mse: 167.4922\n",
            "Epoch 133/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.8193 - mse: 168.5210 - val_loss: 1.8225 - val_mse: 167.1207\n",
            "Epoch 134/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8165 - mse: 168.9055 - val_loss: 1.8233 - val_mse: 165.5244\n",
            "Epoch 135/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8040 - mse: 166.5909 - val_loss: 1.8226 - val_mse: 166.8214\n",
            "Epoch 136/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8204 - mse: 169.0392 - val_loss: 1.8221 - val_mse: 166.3427\n",
            "Epoch 137/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8150 - mse: 168.0126 - val_loss: 1.8218 - val_mse: 167.9060\n",
            "Epoch 138/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8077 - mse: 166.7605 - val_loss: 1.8221 - val_mse: 166.3361\n",
            "Epoch 139/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8132 - mse: 167.5931 - val_loss: 1.8217 - val_mse: 166.3497\n",
            "Epoch 140/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8196 - mse: 168.4790 - val_loss: 1.8218 - val_mse: 168.7367\n",
            "Epoch 141/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8113 - mse: 167.3916 - val_loss: 1.8218 - val_mse: 167.1255\n",
            "Epoch 142/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8273 - mse: 170.3546 - val_loss: 1.8216 - val_mse: 167.6488\n",
            "Epoch 143/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8159 - mse: 167.7041 - val_loss: 1.8218 - val_mse: 166.3936\n",
            "Epoch 144/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8207 - mse: 168.5977 - val_loss: 1.8213 - val_mse: 167.1833\n",
            "Epoch 145/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8266 - mse: 169.7713 - val_loss: 1.8216 - val_mse: 166.6893\n",
            "Epoch 146/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8148 - mse: 168.3150 - val_loss: 1.8218 - val_mse: 165.6344\n",
            "Epoch 147/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8317 - mse: 169.9091 - val_loss: 1.8214 - val_mse: 166.6529\n",
            "Epoch 148/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8069 - mse: 166.5732 - val_loss: 1.8213 - val_mse: 168.0063\n",
            "Epoch 149/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8292 - mse: 170.6144 - val_loss: 1.8212 - val_mse: 167.3473\n",
            "Epoch 150/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8234 - mse: 169.1137 - val_loss: 1.8213 - val_mse: 167.3247\n",
            "Epoch 151/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.8153 - mse: 168.5893 - val_loss: 1.8212 - val_mse: 167.3910\n",
            "Epoch 152/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8150 - mse: 168.3702 - val_loss: 1.8213 - val_mse: 166.5628\n",
            "Epoch 153/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8222 - mse: 169.3078 - val_loss: 1.8211 - val_mse: 167.7028\n",
            "Epoch 154/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8218 - mse: 169.4455 - val_loss: 1.8211 - val_mse: 167.2195\n",
            "Epoch 155/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8121 - mse: 168.0145 - val_loss: 1.8212 - val_mse: 166.7556\n",
            "Epoch 156/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8156 - mse: 167.9606 - val_loss: 1.8211 - val_mse: 167.4992\n",
            "Epoch 157/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8270 - mse: 169.6294 - val_loss: 1.8211 - val_mse: 167.2075\n",
            "Epoch 158/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8206 - mse: 169.4343 - val_loss: 1.8211 - val_mse: 166.9639\n",
            "Epoch 159/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8191 - mse: 169.5054 - val_loss: 1.8211 - val_mse: 167.1233\n",
            "Epoch 160/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8166 - mse: 167.8789 - val_loss: 1.8213 - val_mse: 166.5524\n",
            "Epoch 161/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8059 - mse: 166.7245 - val_loss: 1.8211 - val_mse: 167.0107\n",
            "Epoch 162/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8063 - mse: 167.4745 - val_loss: 1.8211 - val_mse: 167.0107\n",
            "Epoch 163/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8292 - mse: 170.9041 - val_loss: 1.8211 - val_mse: 167.0056\n",
            "Epoch 164/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.8276 - mse: 169.7940 - val_loss: 1.8211 - val_mse: 167.0405\n",
            "219/219 [==============================] - 1s 6ms/step - loss: 1.8038 - mse: 164.9506\n",
            "loss:  1.803763508796692\n",
            "mae:  164.9506378173828\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f97383206a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "0.7번째 지정\n",
            "0.8번째 훈련\n",
            "Epoch 1/500\n",
            "560/560 [==============================] - 14s 22ms/step - loss: 11.2206 - mse: 2166.8503 - val_loss: 7.2574 - val_mse: 977.0223\n",
            "Epoch 2/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 6.4333 - mse: 812.9460 - val_loss: 4.9156 - val_mse: 588.1757\n",
            "Epoch 3/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 4.4390 - mse: 489.9602 - val_loss: 3.2448 - val_mse: 300.7009\n",
            "Epoch 4/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.9969 - mse: 312.5675 - val_loss: 2.5682 - val_mse: 265.9986\n",
            "Epoch 5/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.4612 - mse: 262.9025 - val_loss: 2.3360 - val_mse: 252.1787\n",
            "Epoch 6/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 2.3118 - mse: 255.6937 - val_loss: 2.2614 - val_mse: 228.8141\n",
            "Epoch 7/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.1857 - mse: 246.2669 - val_loss: 2.1949 - val_mse: 208.1802\n",
            "Epoch 8/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 2.1289 - mse: 244.1342 - val_loss: 2.0761 - val_mse: 234.7526\n",
            "Epoch 9/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 2.0277 - mse: 234.8481 - val_loss: 2.0124 - val_mse: 214.2068\n",
            "Epoch 10/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.9881 - mse: 234.0748 - val_loss: 2.0416 - val_mse: 274.4897\n",
            "Epoch 11/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.9704 - mse: 235.8830 - val_loss: 1.9811 - val_mse: 211.2715\n",
            "Epoch 12/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.9134 - mse: 233.7539 - val_loss: 1.9421 - val_mse: 195.9646\n",
            "Epoch 13/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8599 - mse: 226.7137 - val_loss: 1.8725 - val_mse: 201.9592\n",
            "Epoch 14/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.8355 - mse: 227.5782 - val_loss: 1.8064 - val_mse: 205.0629\n",
            "Epoch 15/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7818 - mse: 224.2704 - val_loss: 1.7572 - val_mse: 231.7254\n",
            "Epoch 16/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7569 - mse: 223.9017 - val_loss: 1.7229 - val_mse: 205.4345\n",
            "Epoch 17/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7238 - mse: 224.0570 - val_loss: 1.7128 - val_mse: 212.7153\n",
            "Epoch 18/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.7047 - mse: 223.8276 - val_loss: 1.6718 - val_mse: 226.9143\n",
            "Epoch 19/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.6875 - mse: 224.1257 - val_loss: 1.6720 - val_mse: 203.6200\n",
            "Epoch 20/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.6524 - mse: 220.4947 - val_loss: 1.6619 - val_mse: 239.3350\n",
            "Epoch 21/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.6535 - mse: 224.0528 - val_loss: 1.7692 - val_mse: 189.4573\n",
            "Epoch 22/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.6329 - mse: 219.0019 - val_loss: 1.6159 - val_mse: 208.2668\n",
            "Epoch 23/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.6194 - mse: 221.4624 - val_loss: 1.7778 - val_mse: 243.7454\n",
            "Epoch 24/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.6321 - mse: 223.1882 - val_loss: 1.6622 - val_mse: 254.4033\n",
            "Epoch 25/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.5861 - mse: 218.1283 - val_loss: 1.6677 - val_mse: 200.0089\n",
            "Epoch 26/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.5938 - mse: 218.3987 - val_loss: 1.5662 - val_mse: 232.0397\n",
            "Epoch 27/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.6173 - mse: 226.0607 - val_loss: 1.6186 - val_mse: 207.3235\n",
            "Epoch 28/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.5685 - mse: 218.5286 - val_loss: 1.6034 - val_mse: 196.6939\n",
            "Epoch 29/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.5721 - mse: 220.8008 - val_loss: 1.5917 - val_mse: 201.3179\n",
            "Epoch 30/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.5612 - mse: 219.9750 - val_loss: 1.5679 - val_mse: 199.6264\n",
            "Epoch 31/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.5220 - mse: 221.8069 - val_loss: 1.4978 - val_mse: 221.9565\n",
            "Epoch 32/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.5034 - mse: 219.0216 - val_loss: 1.5117 - val_mse: 223.9554\n",
            "Epoch 33/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.5140 - mse: 221.1247 - val_loss: 1.5161 - val_mse: 204.3204\n",
            "Epoch 34/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.5043 - mse: 218.1734 - val_loss: 1.4978 - val_mse: 223.0290\n",
            "Epoch 35/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.4992 - mse: 218.6587 - val_loss: 1.5640 - val_mse: 244.2950\n",
            "Epoch 36/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.4781 - mse: 218.0614 - val_loss: 1.4688 - val_mse: 218.4651\n",
            "Epoch 37/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.4796 - mse: 219.7286 - val_loss: 1.4707 - val_mse: 214.7982\n",
            "Epoch 38/500\n",
            "560/560 [==============================] - 13s 24ms/step - loss: 1.4673 - mse: 217.7546 - val_loss: 1.4583 - val_mse: 218.9536\n",
            "Epoch 39/500\n",
            "560/560 [==============================] - 15s 27ms/step - loss: 1.4618 - mse: 217.4904 - val_loss: 1.4622 - val_mse: 210.8360\n",
            "Epoch 40/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.4723 - mse: 220.1143 - val_loss: 1.4600 - val_mse: 217.0227\n",
            "Epoch 41/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.4562 - mse: 217.0149 - val_loss: 1.4467 - val_mse: 217.1896\n",
            "Epoch 42/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.4535 - mse: 217.2222 - val_loss: 1.4492 - val_mse: 209.1640\n",
            "Epoch 43/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.4591 - mse: 219.3986 - val_loss: 1.4457 - val_mse: 217.2834\n",
            "Epoch 44/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.4545 - mse: 218.9310 - val_loss: 1.4480 - val_mse: 221.9724\n",
            "Epoch 45/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.4455 - mse: 216.9536 - val_loss: 1.4434 - val_mse: 209.5240\n",
            "Epoch 46/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.4392 - mse: 214.8275 - val_loss: 1.4531 - val_mse: 226.2151\n",
            "Epoch 47/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.4390 - mse: 217.2539 - val_loss: 1.4442 - val_mse: 209.7048\n",
            "Epoch 48/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.4403 - mse: 217.6965 - val_loss: 1.4387 - val_mse: 210.7733\n",
            "Epoch 49/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.4375 - mse: 216.8760 - val_loss: 1.4335 - val_mse: 218.6195\n",
            "Epoch 50/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.4450 - mse: 218.7238 - val_loss: 1.4636 - val_mse: 204.9861\n",
            "Epoch 51/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.4398 - mse: 217.1949 - val_loss: 1.4215 - val_mse: 214.5471\n",
            "Epoch 52/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.4389 - mse: 218.4846 - val_loss: 1.4255 - val_mse: 212.4449\n",
            "Epoch 53/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.4301 - mse: 216.1246 - val_loss: 1.4278 - val_mse: 208.6555\n",
            "Epoch 54/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.4317 - mse: 217.0966 - val_loss: 1.4271 - val_mse: 209.3223\n",
            "Epoch 55/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.4289 - mse: 216.6444 - val_loss: 1.4377 - val_mse: 202.8106\n",
            "Epoch 56/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.4197 - mse: 217.7041 - val_loss: 1.4141 - val_mse: 209.9765\n",
            "Epoch 57/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.4176 - mse: 217.0933 - val_loss: 1.4103 - val_mse: 213.5126\n",
            "Epoch 58/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.4130 - mse: 217.0384 - val_loss: 1.4096 - val_mse: 213.6444\n",
            "Epoch 59/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.4158 - mse: 216.6529 - val_loss: 1.4101 - val_mse: 217.6920\n",
            "Epoch 60/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.4143 - mse: 217.2531 - val_loss: 1.4143 - val_mse: 221.9470\n",
            "Epoch 61/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.4180 - mse: 217.3012 - val_loss: 1.4227 - val_mse: 207.6792\n",
            "Epoch 62/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.4046 - mse: 214.7882 - val_loss: 1.4141 - val_mse: 221.8895\n",
            "Epoch 63/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.4033 - mse: 215.5419 - val_loss: 1.4002 - val_mse: 217.8911\n",
            "Epoch 64/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.4082 - mse: 216.5608 - val_loss: 1.4029 - val_mse: 214.7514\n",
            "Epoch 65/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.4062 - mse: 216.7397 - val_loss: 1.4001 - val_mse: 212.5587\n",
            "Epoch 66/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3975 - mse: 215.2615 - val_loss: 1.4002 - val_mse: 216.2780\n",
            "Epoch 67/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.3994 - mse: 214.8968 - val_loss: 1.4022 - val_mse: 215.1442\n",
            "Epoch 68/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.3955 - mse: 214.2495 - val_loss: 1.4029 - val_mse: 206.7667\n",
            "Epoch 69/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.3990 - mse: 214.9398 - val_loss: 1.3992 - val_mse: 213.1979\n",
            "Epoch 70/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.3907 - mse: 212.8479 - val_loss: 1.3995 - val_mse: 214.9670\n",
            "Epoch 71/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.3990 - mse: 214.7696 - val_loss: 1.4009 - val_mse: 216.6135\n",
            "Epoch 72/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.4003 - mse: 216.4437 - val_loss: 1.3988 - val_mse: 216.2835\n",
            "Epoch 73/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.4044 - mse: 217.0313 - val_loss: 1.3964 - val_mse: 215.2149\n",
            "Epoch 74/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.3953 - mse: 214.6200 - val_loss: 1.3971 - val_mse: 214.8611\n",
            "Epoch 75/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.4128 - mse: 218.7147 - val_loss: 1.4025 - val_mse: 220.8590\n",
            "Epoch 76/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.4008 - mse: 217.0185 - val_loss: 1.4001 - val_mse: 209.4497\n",
            "Epoch 77/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.4042 - mse: 217.4485 - val_loss: 1.3955 - val_mse: 213.1939\n",
            "Epoch 78/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.4044 - mse: 218.0879 - val_loss: 1.3959 - val_mse: 211.1146\n",
            "Epoch 79/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.4001 - mse: 215.9594 - val_loss: 1.3973 - val_mse: 210.7481\n",
            "Epoch 80/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3990 - mse: 216.1502 - val_loss: 1.3966 - val_mse: 213.6866\n",
            "Epoch 81/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.4037 - mse: 216.9004 - val_loss: 1.3936 - val_mse: 214.2336\n",
            "Epoch 82/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3953 - mse: 215.1490 - val_loss: 1.3944 - val_mse: 213.1559\n",
            "Epoch 83/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.4004 - mse: 216.7365 - val_loss: 1.3975 - val_mse: 217.0277\n",
            "Epoch 84/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.3998 - mse: 216.4213 - val_loss: 1.3923 - val_mse: 213.2018\n",
            "Epoch 85/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.4013 - mse: 216.7254 - val_loss: 1.3973 - val_mse: 219.9068\n",
            "Epoch 86/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3939 - mse: 215.5789 - val_loss: 1.3960 - val_mse: 219.5807\n",
            "Epoch 87/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3982 - mse: 216.4975 - val_loss: 1.3915 - val_mse: 211.4674\n",
            "Epoch 88/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3966 - mse: 215.0771 - val_loss: 1.3939 - val_mse: 217.5678\n",
            "Epoch 89/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.3970 - mse: 216.6373 - val_loss: 1.3948 - val_mse: 211.8730\n",
            "Epoch 90/500\n",
            "560/560 [==============================] - 14s 24ms/step - loss: 1.4055 - mse: 218.2282 - val_loss: 1.3933 - val_mse: 212.1843\n",
            "Epoch 91/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.4024 - mse: 217.4167 - val_loss: 1.3942 - val_mse: 208.9755\n",
            "Epoch 92/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.3996 - mse: 216.8720 - val_loss: 1.3915 - val_mse: 212.9726\n",
            "Epoch 93/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.4018 - mse: 217.6056 - val_loss: 1.3912 - val_mse: 215.0828\n",
            "Epoch 94/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.3929 - mse: 215.0738 - val_loss: 1.3892 - val_mse: 213.7371\n",
            "Epoch 95/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.3817 - mse: 213.2919 - val_loss: 1.3886 - val_mse: 214.6034\n",
            "Epoch 96/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3863 - mse: 213.8698 - val_loss: 1.3890 - val_mse: 211.7596\n",
            "Epoch 97/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.3892 - mse: 214.5976 - val_loss: 1.3901 - val_mse: 213.7680\n",
            "Epoch 98/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.3907 - mse: 214.8641 - val_loss: 1.3909 - val_mse: 211.9067\n",
            "Epoch 99/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.3889 - mse: 214.7491 - val_loss: 1.3878 - val_mse: 214.1535\n",
            "Epoch 100/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.3891 - mse: 214.8944 - val_loss: 1.3903 - val_mse: 209.5221\n",
            "Epoch 101/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.3898 - mse: 215.3996 - val_loss: 1.3887 - val_mse: 211.9158\n",
            "Epoch 102/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3908 - mse: 215.7859 - val_loss: 1.3905 - val_mse: 212.2581\n",
            "Epoch 103/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.3900 - mse: 215.2988 - val_loss: 1.3883 - val_mse: 210.6194\n",
            "Epoch 104/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.3931 - mse: 216.2146 - val_loss: 1.3870 - val_mse: 210.7771\n",
            "Epoch 105/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.3876 - mse: 214.8208 - val_loss: 1.3865 - val_mse: 211.1125\n",
            "Epoch 106/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.3946 - mse: 216.9838 - val_loss: 1.3861 - val_mse: 213.8550\n",
            "Epoch 107/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3819 - mse: 213.9469 - val_loss: 1.3868 - val_mse: 210.6634\n",
            "Epoch 108/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.3928 - mse: 216.5960 - val_loss: 1.3876 - val_mse: 217.8951\n",
            "Epoch 109/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3914 - mse: 216.6848 - val_loss: 1.3854 - val_mse: 213.1281\n",
            "Epoch 110/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.3882 - mse: 215.2587 - val_loss: 1.3860 - val_mse: 214.6336\n",
            "Epoch 111/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.3880 - mse: 215.6184 - val_loss: 1.3858 - val_mse: 214.3359\n",
            "Epoch 112/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.3877 - mse: 215.6098 - val_loss: 1.3864 - val_mse: 214.9112\n",
            "Epoch 113/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.3836 - mse: 214.1664 - val_loss: 1.3877 - val_mse: 211.4855\n",
            "Epoch 114/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3905 - mse: 216.4616 - val_loss: 1.3851 - val_mse: 214.6793\n",
            "Epoch 115/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.3896 - mse: 216.4101 - val_loss: 1.3854 - val_mse: 212.8953\n",
            "Epoch 116/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3802 - mse: 214.0402 - val_loss: 1.3852 - val_mse: 215.5574\n",
            "Epoch 117/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.3901 - mse: 217.0200 - val_loss: 1.3848 - val_mse: 212.8644\n",
            "Epoch 118/500\n",
            "560/560 [==============================] - 14s 25ms/step - loss: 1.3864 - mse: 214.9013 - val_loss: 1.3852 - val_mse: 214.7183\n",
            "Epoch 119/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3890 - mse: 216.3688 - val_loss: 1.3850 - val_mse: 212.3816\n",
            "Epoch 120/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3790 - mse: 213.5836 - val_loss: 1.3859 - val_mse: 210.3795\n",
            "Epoch 121/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.3963 - mse: 217.3991 - val_loss: 1.3849 - val_mse: 213.2425\n",
            "Epoch 122/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.3889 - mse: 216.0131 - val_loss: 1.3845 - val_mse: 213.8675\n",
            "Epoch 123/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.3839 - mse: 214.3408 - val_loss: 1.3844 - val_mse: 213.4220\n",
            "Epoch 124/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3788 - mse: 213.3874 - val_loss: 1.3845 - val_mse: 213.7850\n",
            "Epoch 125/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3872 - mse: 215.8891 - val_loss: 1.3844 - val_mse: 212.9511\n",
            "Epoch 126/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.3889 - mse: 216.3762 - val_loss: 1.3844 - val_mse: 213.9869\n",
            "Epoch 127/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.3902 - mse: 216.3937 - val_loss: 1.3844 - val_mse: 212.7251\n",
            "Epoch 128/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.3872 - mse: 215.8490 - val_loss: 1.3842 - val_mse: 213.2272\n",
            "Epoch 129/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.3852 - mse: 215.1361 - val_loss: 1.3843 - val_mse: 212.3033\n",
            "Epoch 130/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3905 - mse: 215.9017 - val_loss: 1.3842 - val_mse: 213.2314\n",
            "Epoch 131/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3859 - mse: 215.1701 - val_loss: 1.3842 - val_mse: 214.3859\n",
            "Epoch 132/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3924 - mse: 217.8226 - val_loss: 1.3842 - val_mse: 213.4043\n",
            "Epoch 133/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.3833 - mse: 215.0880 - val_loss: 1.3842 - val_mse: 213.3738\n",
            "Epoch 134/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3933 - mse: 216.8789 - val_loss: 1.3841 - val_mse: 213.2538\n",
            "Epoch 135/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3839 - mse: 215.0229 - val_loss: 1.3841 - val_mse: 213.1063\n",
            "Epoch 136/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3910 - mse: 215.9780 - val_loss: 1.3841 - val_mse: 213.7660\n",
            "Epoch 137/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3899 - mse: 216.5553 - val_loss: 1.3841 - val_mse: 213.5896\n",
            "Epoch 138/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.3849 - mse: 215.4583 - val_loss: 1.3841 - val_mse: 213.9935\n",
            "Epoch 139/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3824 - mse: 214.9675 - val_loss: 1.3841 - val_mse: 212.9792\n",
            "Epoch 140/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.3888 - mse: 216.6631 - val_loss: 1.3840 - val_mse: 213.5005\n",
            "Epoch 141/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.3907 - mse: 216.6589 - val_loss: 1.3841 - val_mse: 213.2737\n",
            "Epoch 142/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.3873 - mse: 215.4069 - val_loss: 1.3842 - val_mse: 212.6104\n",
            "Epoch 143/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 1.3860 - mse: 215.0290 - val_loss: 1.3840 - val_mse: 213.3123\n",
            "Epoch 144/500\n",
            "560/560 [==============================] - 15s 26ms/step - loss: 1.3830 - mse: 215.4379 - val_loss: 1.3840 - val_mse: 213.3209\n",
            "Epoch 145/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3886 - mse: 216.1692 - val_loss: 1.3840 - val_mse: 213.2750\n",
            "Epoch 146/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3925 - mse: 217.0995 - val_loss: 1.3840 - val_mse: 213.3441\n",
            "Epoch 147/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3832 - mse: 214.6240 - val_loss: 1.3840 - val_mse: 213.3378\n",
            "Epoch 148/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3849 - mse: 216.0134 - val_loss: 1.3840 - val_mse: 213.3540\n",
            "Epoch 149/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3886 - mse: 216.3820 - val_loss: 1.3840 - val_mse: 213.2779\n",
            "Epoch 150/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3913 - mse: 216.8180 - val_loss: 1.3840 - val_mse: 213.2275\n",
            "Epoch 151/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3818 - mse: 215.1747 - val_loss: 1.3840 - val_mse: 213.2646\n",
            "Epoch 152/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3805 - mse: 213.7175 - val_loss: 1.3840 - val_mse: 213.2502\n",
            "Epoch 153/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.3861 - mse: 216.0398 - val_loss: 1.3840 - val_mse: 213.2468\n",
            "Epoch 154/500\n",
            "560/560 [==============================] - 12s 21ms/step - loss: 1.3755 - mse: 213.0403 - val_loss: 1.3840 - val_mse: 213.2515\n",
            "Epoch 155/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 1.3901 - mse: 216.8055 - val_loss: 1.3840 - val_mse: 213.2495\n",
            "Epoch 156/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 1.3840 - mse: 214.6236 - val_loss: 1.3840 - val_mse: 213.2571\n",
            "219/219 [==============================] - 1s 6ms/step - loss: 1.3752 - mse: 210.5809\n",
            "loss:  1.375173568725586\n",
            "mae:  210.5809326171875\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9758387048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "0.8번째 지정\n",
            "0.9번째 훈련\n",
            "Epoch 1/500\n",
            "560/560 [==============================] - 14s 22ms/step - loss: 11.9322 - mse: 4974.7898 - val_loss: 5.7572 - val_mse: 2161.2917\n",
            "Epoch 2/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 18.3491 - mse: 580145.2441 - val_loss: 11.3492 - val_mse: 2735.1782\n",
            "Epoch 3/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 11.0304 - mse: 2619.2128 - val_loss: 9.5789 - val_mse: 2182.8552\n",
            "Epoch 4/500\n",
            "560/560 [==============================] - 12s 22ms/step - loss: 8.8722 - mse: 3110.8738 - val_loss: 11.7214 - val_mse: 1350.8192\n",
            "Epoch 5/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 7.3853 - mse: 4465.1765 - val_loss: 5.8262 - val_mse: 2064.8186\n",
            "Epoch 6/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 5.8971 - mse: 2255.4720 - val_loss: 5.8918 - val_mse: 1921.8801\n",
            "Epoch 7/500\n",
            "560/560 [==============================] - 13s 22ms/step - loss: 8.1718 - mse: 23539.0322 - val_loss: 7.3436 - val_mse: 6665.5405\n",
            "Epoch 8/500\n",
            "560/560 [==============================] - 14s 25ms/step - loss: 6.9650 - mse: 4333.9401 - val_loss: 7.8404 - val_mse: 3215.6365\n",
            "Epoch 9/500\n",
            "560/560 [==============================] - 13s 23ms/step - loss: 7.1719 - mse: 5649.9245 - val_loss: 5.9300 - val_mse: 2422.9619\n",
            "219/219 [==============================] - 2s 7ms/step - loss: 5.8939 - mse: 2436.5854\n",
            "loss:  5.893887996673584\n",
            "mae:  2436.58544921875\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f97374969d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "0.9번째 지정\n",
            "(ง˙∇˙)ว {오늘 안에 조지고만다!!!]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}